{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb26e65-1762-47db-8a66-e80da35bbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import ta\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"dqn_trading_transformer\"\n",
    "log_folder = \"./\"\n",
    "candles_dir = \"../candles/\"\n",
    "\n",
    "training_parallel = 4\n",
    "warmup_parallel = 8\n",
    "warmup_steps = 5000\n",
    "\n",
    "\n",
    "loss_prio_sample_mult = 32\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "memory_size = 300000\n",
    "lr  = 0.0002\n",
    "seq_len = 600\n",
    "\n",
    "soft_reward_inc = 1.05\n",
    "comission = 10/100000\n",
    "\n",
    "resume = True\n",
    "#resume = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433ff867-691a-4d63-8647-a4dcc04f95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_to_x(sample):\n",
    "        \n",
    "                current_close = sample[-1].c\n",
    "\n",
    "                prev_close = [candle.c for candle in sample]\n",
    "                prev_high = [candle.h for candle in sample]\n",
    "                prev_low = [candle.l for candle in sample]\n",
    "                \n",
    "\n",
    "                prev_sma21 = [candle.sma21 for candle in sample]\n",
    "                prev_sma200 = [candle.sma200 for candle in sample]\n",
    "                \n",
    "                \n",
    "                prev_sma21_relative = [(prev_close[o] - prev_sma21[o]) / prev_sma21[o]*100 for o in range(seq_len)]\n",
    "                prev_sma200_relative = [(prev_close[o] - prev_sma200[o]) / prev_sma200[o]*100 for o in range(seq_len)]\n",
    "\n",
    "                prev_close_relative = [0] + [(prev_close[o+1] - prev_close[o]) / prev_close[o]*1000 for o in range(seq_len-1)]\n",
    "                prev_high_relative = [(prev_close[o] - prev_high[o]) / prev_close[o]*1000 for o in range(seq_len)]\n",
    "                prev_low_relative = [(prev_close[o] - prev_low[o]) / prev_close[o]*1000 for o in range(seq_len)]\n",
    "                \n",
    "\n",
    "                \n",
    "                prev_rsi_14 = [candle.rsi14 for candle in sample]\n",
    "                \n",
    "\n",
    "                x = []\n",
    "                for o in range(len(prev_close)):\n",
    "                    ts = []\n",
    "\n",
    "                    \n",
    "                    ts.append(prev_close_relative[o])\n",
    "                    ts.append(prev_high_relative[o])\n",
    "                    ts.append(prev_low_relative[o])\n",
    "                    \n",
    "                    ts.append(prev_sma21_relative[o])\n",
    "                    ts.append(prev_sma200_relative[o])\n",
    "                    \n",
    "                    ts.append(prev_rsi_14[o])\n",
    "\n",
    "                    x.append(ts)\n",
    "\n",
    "                x = np.array(x)\n",
    "                return x\n",
    "        \n",
    "\n",
    "\n",
    "def Load(file):\n",
    "    f = open(file, \"rb\")\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_interval = 4*24 # environment logs daily returns\n",
    "\n",
    "class candle_class:\n",
    "    pass\n",
    "  \n",
    "order_value = 1000\n",
    "\n",
    "\n",
    "class environment():\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def _next_observation(self):\n",
    "            candles = self.candles[self.current_step - seq_len + 1:self.current_step + 1]\n",
    "            \n",
    "            inference_data = sample_to_x(candles)\n",
    "            \n",
    "            return inference_data, np.array([self.position, math.tanh(self.current_win)])\n",
    "\n",
    "  \n",
    "  def reset(self, first_reset = False):\n",
    "    self.candles = None\n",
    "    candles_files = os.listdir(candles_dir)\n",
    "    use_file = candles_dir+random.choice(candles_files)\n",
    "    #print(use_file)\n",
    "    self.candles = Load(use_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.current_step = 200+seq_len if first_reset == False else random.randint(200+seq_len,len(self.candles) - 1000)\n",
    "    self.position = 0\n",
    "    self.entry_price = 0\n",
    "    self.win = 0\n",
    "    self.current_win = 0\n",
    "    self.startindex = self.current_step\n",
    "    self.last_reward = 0\n",
    "    self.reward_tr_given = 0\n",
    "    self.reward_since_last_log = 0\n",
    "    self.closed_trades_since_last_log = 0\n",
    "\n",
    "    return self._next_observation()\n",
    "\n",
    "  \n",
    "  def close(self):\n",
    "        self.win -= comission * order_value / 2\n",
    "        self.position = 0\n",
    "        self.win+=self.current_win - self.reward_tr_given\n",
    "        self.reward_tr_given = 0\n",
    "        self.current_win = 0\n",
    "        self.closed_trades_since_last_log+=1\n",
    "        \n",
    "        \n",
    "  def entry(self):\n",
    "        self.entry_price = self.candles[self.current_step].c\n",
    "        self.win -= comission * order_value / 2\n",
    "\n",
    "  def step(self, action):\n",
    "    action+=1 # disable no position\n",
    "    \n",
    "    if action == 0:\n",
    "        if self.position != 0:\n",
    "            self.close()\n",
    "    \n",
    "    if action == 1:\n",
    "      #short\n",
    "      if self.position == 1:\n",
    "        self.close()\n",
    "\n",
    "      if self.position == -1:\n",
    "        pass\n",
    "      else:\n",
    "        self.position = -1\n",
    "        self.entry()\n",
    "        \n",
    "    if action == 2:\n",
    "      #long\n",
    "      if self.position == -1:\n",
    "        self.close()\n",
    "\n",
    "      if self.position == 1:\n",
    "        pass\n",
    "      else:\n",
    "        self.position = 1\n",
    "        self.entry()\n",
    "        \n",
    "    self.current_step += 1\n",
    "    if self.position != 0:\n",
    "      current_price = self.candles[self.current_step].c\n",
    "      entry = self.entry_price\n",
    "      diff = (current_price - entry) / entry * order_value\n",
    "\n",
    "      if self.position == 1:\n",
    "        self.current_win = diff\n",
    "      if self.position == -1:\n",
    "        self.current_win = -diff\n",
    "\n",
    "        \n",
    "    diff = self.current_win - self.reward_tr_given\n",
    "    reward_inc = diff / soft_reward_inc\n",
    "    self.reward_tr_given += reward_inc\n",
    "    self.win += reward_inc\n",
    "    \n",
    "    reward_raw = self.win# + self.current_win  # sparse reward disabled\n",
    "    reward = reward_raw - self.last_reward\n",
    "    self.last_reward = reward_raw\n",
    "    reward = max(min(reward, 10), -10)\n",
    "    \n",
    "    \n",
    "    done = self.current_step == len(self.candles) -1\n",
    "    \n",
    "    if (self.current_step - self.startindex) % log_interval == 0:\n",
    "        log_reward = reward_raw - self.reward_since_last_log\n",
    "        log_reward = max(min(log_reward, 200), -200)\n",
    "        self.reward_since_last_log = reward_raw \n",
    "        file2 = open(log_folder+\"logs/r2_log.txt\", \"a\")  \n",
    "        file2.write(str(log_reward))\n",
    "        file2.write(\"\\n\")\n",
    "        file2.close()\n",
    "        \n",
    "        \n",
    "        file2 = open(log_folder+\"logs/num_trades_per_day.txt\", \"a\")  \n",
    "        file2.write(str(self.closed_trades_since_last_log))\n",
    "        file2.write(\"\\n\")\n",
    "        file2.close()\n",
    "        \n",
    "        \n",
    "        self.closed_trades_since_last_log = 0\n",
    "    \n",
    "    obs = self._next_observation()\n",
    "    return obs, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc54f552-1e9d-42a7-9877-12297ea740d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 600, 6)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 600, 64)      1216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600, 70)      0           conv1d[0][0]                     \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 600, 32)      2272        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 600, 32)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 600, 32)      64          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 600, 128)     20608       layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600, 160)     0           conv1d_1[0][0]                   \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 600, 64)      10304       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 600, 64)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 600, 64)      128         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 600, 128)     41088       layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600, 192)     0           conv1d_2[0][0]                   \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 600, 192)     384         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 600, 128)     24704       layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 600, 128)     82048       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 600, 128)     0           conv1d_3[0][0]                   \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 600, 128)     82048       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 600, 128)     0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 600, 128)     82048       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 600, 128)     0           conv1d_5[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 600, 128)     82048       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 600, 128)     0           conv1d_6[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 600, 128)     82048       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 600, 128)     0           conv1d_7[0][0]                   \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 600, 128)     82048       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 600, 128)     0           conv1d_8[0][0]                   \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 600, 128)     82048       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 600, 128)     0           conv1d_9[0][0]                   \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 600, 32)      4128        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 600, 32)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 600, 16)      528         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 600, 16)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 6)            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 9600)         0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 6)            0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 9608)         0           input_2[0][0]                    \n",
      "                                                                 flatten[0][0]                    \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         9839616     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1049600     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1049600     leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            2048        leaky_re_lu_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,670,224\n",
      "Trainable params: 13,670,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, model,\n",
    "                 n_actions,\n",
    "                 memory_size = 10000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.0005), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =32,\n",
    "                 name = \"dqn1\",\n",
    "                 target_model_sync = 1000,\n",
    "                 exploration = 0.01\n",
    "                ):\n",
    "        self.exploration = exploration\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.memory_size = memory_size\n",
    "        self.optimizer = optimizer\n",
    "        self.m1 = np.eye(self.n_actions, dtype=\"float32\")\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_sync = target_model_sync\n",
    "   \n",
    "        self.memory = deque(maxlen = self.memory_size)\n",
    "      \n",
    "    \n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "      \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.name)\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.name, overwrite = True)\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def model_call(self, x):\n",
    "        x1, x2 = x\n",
    "        return tf.math.argmax(self.model([x1,x2]), axis = 1)\n",
    "    \n",
    "    def select_actions(self, current_states, positions):\n",
    "        \n",
    "        ret = self.model_call([current_states, positions])\n",
    "        return ret\n",
    "\n",
    "\n",
    "        \n",
    "    def observe_sasrt(self, state, action, next_state, reward, terminal):\n",
    "        self.memory.append([state, action, reward, 1-int(terminal), next_state, 1])\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def get_target_q(self, next_states, rewards, terminals):\n",
    "        estimated_q_values_next = self.target_model(next_states)\n",
    "        q_batch = tf.math.reduce_max(estimated_q_values_next, axis=1)\n",
    "        target_q_values = q_batch * self.gamma * terminals + rewards\n",
    "        return target_q_values\n",
    "\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def tstep(self, data):\n",
    "        states, next_states, rewards, terminals, masks, idx = data\n",
    "        target_q_values = self.get_target_q(next_states, rewards, terminals)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            estimated_q_values = tf.math.reduce_sum(self.model(states, training=True) * masks, axis=1)\n",
    "            loss_e = tf.math.square(target_q_values - estimated_q_values)\n",
    "            loss = tf.reduce_mean(loss_e)\n",
    "        \n",
    "        \n",
    "        gradient = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        return loss, tf.reduce_mean(estimated_q_values), loss_e, idx\n",
    "    \n",
    "    \n",
    "    def data_get_func(self):\n",
    "        idx = np.random.randint(0, len(self.memory), self.batch_size*loss_prio_sample_mult)\n",
    "        for i in idx:\n",
    "            self.memory[i][-1]*=1.001\n",
    "        sarts_batch = [self.memory[i] for i in idx]\n",
    "        sorted_batch_idx = sorted(zip(sarts_batch, idx), key=lambda x:x[0][-1])[0:self.batch_size]\n",
    "        sarts_batch = [i[0] for i in sorted_batch_idx]\n",
    "        sorted_idx = [i[1] for i in sorted_batch_idx]\n",
    "        \n",
    "        states = [x[0] for x in sarts_batch]\n",
    "        states_1 = np.array([x[0] for x in states], dtype=\"float32\")\n",
    "        states_2 = np.array([x[1] for x in states], dtype=\"float32\")\n",
    "        \n",
    "        actions = [x[1] for x in sarts_batch]\n",
    "        rewards = np.array([x[2] for x in sarts_batch], dtype=\"float32\")\n",
    "        terminals = np.array([x[3] for x in sarts_batch], dtype=\"float32\")\n",
    "        \n",
    "        next_states = [x[4] for x in sarts_batch]\n",
    "        next_states_1 = np.array([x[0] for x in next_states], dtype=\"float32\")\n",
    "        next_states_2 = np.array([x[1] for x in next_states], dtype=\"float32\")\n",
    "        \n",
    "        masks = self.m1[actions]\n",
    "        return [states_1, states_2], [next_states_1, next_states_2], rewards, terminals, masks, sorted_idx\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.total_steps_trained+=1\n",
    "        if self.total_steps_trained % self.target_model_sync == 0:\n",
    "            self.copy_weights()\n",
    "\n",
    "           \n",
    "        data = self.data_get_func()\n",
    "        result= self.tstep(data)\n",
    "    \n",
    "        map_loss = [x.numpy() for x in result[2]]\n",
    "        map_idx =[x.numpy() for x in result[3]]\n",
    "#        print(map_loss, map_idx)\n",
    "\n",
    "        for l, i in zip(map_loss,map_idx):\n",
    "                self.memory[i][-1] = l\n",
    "\n",
    "        \n",
    "        return  result[0:2]\n",
    "    \n",
    "    \n",
    "    def train(self, num_steps, envs, log_interval = 1000, warmup = 0, train_steps_per_step = 1):\n",
    "        self.total_steps_trained = -1\n",
    "\n",
    "        num_envs = len(envs)\n",
    "        states = [x.reset(True) for x in envs]\n",
    "        \n",
    "        current_episode_reward_sum = 0\n",
    "        times= deque(maxlen=10)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.longs = 0\n",
    "        self.shorts = 0\n",
    "\n",
    "        self.total_rewards = []\n",
    "        self.losses = [0]\n",
    "        self.q_v = [0]\n",
    "        \n",
    "        def save_current_run():\n",
    "            self.save_weights()\n",
    "            file = open(log_folder+\"logs/loss_log.txt\", \"a\")  \n",
    "            #for loss in self.losses:\n",
    "                        #file.write(str(loss))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.losses)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/qv_log.txt\", \"a\")  \n",
    "            #for qv in self.q_v:\n",
    "                        #file.write(str(qv))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.q_v)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/rewards_log.txt\", \"a\")  \n",
    "            #for total_reward in self.total_rewards:\n",
    "                        #file.write(str(total_reward))\n",
    "                        #file.write(\"\\n\")\n",
    "                    \n",
    "            file.write(str(np.mean(self.total_rewards)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "            self.total_rewards = []\n",
    "            self.losses = [0]\n",
    "            self.q_v = [0]\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_steps):\n",
    "                if i % log_interval == 0:\n",
    "                    progbar = tf.keras.utils.Progbar(log_interval, interval=0.05, stateful_metrics = [\"reward sum\", \"t\", \"l/s\"])\n",
    "                    self.longs = 0\n",
    "                    self.shorts = 0\n",
    "\n",
    "\n",
    "                states_1 = np.array([x[0] for x in states])\n",
    "                states_2 = np.array([x[1] for x in states])\n",
    "                actions = self.select_actions(states_1, states_2)\n",
    "                for action in actions:\n",
    "                    if action == 0:\n",
    "                        self.shorts+=1\n",
    "                    elif action == 1:\n",
    "                        self.longs+=1\n",
    "\n",
    "                sasrt_pairs = []\n",
    "                for index in range(num_envs):\n",
    "                    sasrt_pairs.append([states[index], actions[index]]+[x for x in envs[index].step(actions[index])])\n",
    "\n",
    "                next_states = [x[2] for x in sasrt_pairs]\n",
    "\n",
    "                reward = [x[3] for x in sasrt_pairs]\n",
    "                current_episode_reward_sum += np.sum(reward)\n",
    "\n",
    "                self.total_rewards.extend(reward)\n",
    "\n",
    "                for index, o in enumerate(sasrt_pairs):\n",
    "                    #print(o)\n",
    "                    if o[4] == True:\n",
    "                        next_states[index] = envs[index].reset()\n",
    "                    self.observe_sasrt(o[0], o[1], o[2], o[3], o[4])\n",
    "\n",
    "                states = next_states\n",
    "                if i > warmup:\n",
    "                    for _ in range(train_steps_per_step):\n",
    "                        loss, q = self.update_parameters()\n",
    "                        self.losses.append(loss.numpy())\n",
    "                        self.q_v.append(q.numpy())\n",
    "                else:\n",
    "                    loss, q = 0, 0\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed = (end_time - start_time) * 1000\n",
    "                times.append(elapsed)\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "                if (i+1) % log_interval == 0:\n",
    "                    #print(\"-----------\")\n",
    "                    #print(\"l:\", np.mean(self.losses))\n",
    "                    #print(\"q:\", np.mean(self.q_v))\n",
    "                    #print(\"reward sum\", current_episode_reward_sum)\n",
    "                    #print(\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts))\n",
    "                    #print(\"t\", np.mean(times))\n",
    "                    #print(\"-----------\")\n",
    "                    save_current_run()\n",
    "\n",
    "                progbar.update(i%log_interval+1, values = \n",
    "                               [(\"loss\", np.mean(self.losses[-train_steps_per_step:])),\n",
    "                                (\"mean q\", np.mean(self.q_v[-train_steps_per_step:])),\n",
    "                                (\"rewards\", np.mean(reward)),\n",
    "                                (\"reward sum\", current_episode_reward_sum),\n",
    "                                (\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts)),\n",
    "                                (\"t\", np.mean(times))])\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nbreak!\")\n",
    "        \n",
    "        save_current_run()\n",
    "            \n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if True:\n",
    "  inputs_1 = tf.keras.layers.Input(shape = (seq_len, 6))\n",
    "  inputs_pos = tf.keras.layers.Input(shape = (2))\n",
    "\n",
    "  x = inputs_1\n",
    "\n",
    "  x2 = tf.keras.layers.Conv1D(64, 3,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Concatenate()([x2,x])\n",
    "\n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Concatenate()([x2,x])\n",
    "\n",
    "  x = tf.keras.layers.Dense(64)(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Concatenate()([x2,x])\n",
    "  x = tf.keras.layers.LayerNormalization()(x)\n",
    "    \n",
    "  x = tf.keras.layers.Dense(128,activation = \"relu\")(x) \n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  \n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "  x = tf.keras.layers.Dense(16)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "  last_candle = tf.keras.layers.Reshape((6,))(inputs_1[:,-1])\n",
    "\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  \n",
    "  \n",
    "  x = tf.keras.layers.Concatenate()([inputs_pos, x, last_candle])\n",
    "  \n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "          \n",
    "  outputs = tf.keras.layers.Dense(2, activation = \"linear\", use_bias=False, dtype=\"float32\")(x)\n",
    "  model = tf.keras.Model([inputs_1,inputs_pos], outputs)\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f308778d-9f78-4241-837d-4cc0a20eb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr, clipvalue = 0.2)\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model = model, \n",
    "    n_actions = 2, \n",
    "    memory_size = memory_size, \n",
    "    gamma=gamma,\n",
    "    optimizer = opt,\n",
    "    batch_size = batch_size, \n",
    "    target_model_sync = 500,\n",
    "    exploration = 0.02,\n",
    "    name=log_folder+name+\".h5\")\n",
    "\n",
    "if resume:\n",
    "\tprint(\"loading weights...\")\n",
    "\tagent.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6d1ee-e370-411c-99a8-9309c5acc113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_parallel = warmup_parallel\n",
    "envs = [environment() for _ in range(num_parallel)]\n",
    "\n",
    "print(\"warmup...\")\n",
    "\n",
    "n = int(warmup_steps)\n",
    "#n = int(100)\n",
    "agent.train(num_steps = n, envs = envs, warmup = n, log_interval = n, train_steps_per_step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1fdbe-6e3a-45b3-821c-03a10e6ae936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 210s 210ms/step - loss: 0.0051 - mean q: 1.3223 - rewards: -0.0505 - reward sum: -714.0094 - l/s: -0.2194 - t: 222.7545\n",
      "1000/1000 [==============================] - 369s 369ms/step - loss: 0.0033 - mean q: 1.2867 - rewards: 0.0016 - reward sum: -707.6947 - l/s: -0.0805 - t: 301.2539\n",
      "1000/1000 [==============================] - 1736s 2s/step - loss: 0.0243 - mean q: 1.2642 - rewards: -0.0751 - reward sum: -1008.0441 - l/s: 0.0830 - t: 339.2073\n",
      "1000/1000 [==============================] - 1756s 2s/step - loss: 0.0199 - mean q: 1.2269 - rewards: -0.0536 - reward sum: -1222.4237 - l/s: -0.0180 - t: 226.5647\n",
      "1000/1000 [==============================] - 237s 237ms/step - loss: 0.0084 - mean q: 1.2032 - rewards: -0.0105 - reward sum: -1264.3701 - l/s: 0.0755 - t: 238.4602\n",
      "1000/1000 [==============================] - 251s 251ms/step - loss: 0.0054 - mean q: 1.1657 - rewards: -0.0069 - reward sum: -1291.8758 - l/s: -0.0945 - t: 246.9732\n",
      "1000/1000 [==============================] - 250s 250ms/step - loss: 0.0051 - mean q: 1.1516 - rewards: -0.0254 - reward sum: -1393.5221 - l/s: -0.0540 - t: 252.1111\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 0.0039 - mean q: 1.1426 - rewards: -0.0114 - reward sum: -1439.0914 - l/s: 0.1235 - t: 226.7418\n",
      "1000/1000 [==============================] - 232s 232ms/step - loss: 0.0060 - mean q: 1.1184 - rewards: -0.0062 - reward sum: -1463.9589 - l/s: -0.1230 - t: 245.2972\n",
      "1000/1000 [==============================] - 252s 252ms/step - loss: 0.0060 - mean q: 1.1007 - rewards: -0.0200 - reward sum: -1543.9038 - l/s: 0.0385 - t: 243.3627\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0051 - mean q: 1.0725 - rewards: -0.0156 - reward sum: -1606.4478 - l/s: 0.0050 - t: 246.6105\n",
      "1000/1000 [==============================] - 241s 241ms/step - loss: 0.0087 - mean q: 1.0523 - rewards: -0.0026 - reward sum: -1616.9223 - l/s: 0.0015 - t: 253.5382\n",
      "1000/1000 [==============================] - 251s 251ms/step - loss: 0.0025 - mean q: 1.0295 - rewards: -0.0084 - reward sum: -1650.4479 - l/s: 0.0575 - t: 247.1081\n",
      "1000/1000 [==============================] - 254s 254ms/step - loss: 0.0032 - mean q: 1.0020 - rewards: -0.0385 - reward sum: -1804.3595 - l/s: -0.0545 - t: 242.3383\n",
      "1000/1000 [==============================] - 245s 245ms/step - loss: 0.0036 - mean q: 0.9962 - rewards: 0.0051 - reward sum: -1783.9501 - l/s: -0.2069 - t: 253.0005\n",
      "1000/1000 [==============================] - 247s 247ms/step - loss: 0.0052 - mean q: 0.9876 - rewards: -0.0331 - reward sum: -1916.4507 - l/s: -0.0250 - t: 243.8666\n",
      "1000/1000 [==============================] - 255s 255ms/step - loss: 0.0073 - mean q: 0.9670 - rewards: 0.0140 - reward sum: -1860.4314 - l/s: -0.0480 - t: 259.7083\n",
      "1000/1000 [==============================] - 261s 261ms/step - loss: 0.0048 - mean q: 0.9643 - rewards: 0.0221 - reward sum: -1772.1232 - l/s: 0.1150 - t: 261.7921\n",
      "1000/1000 [==============================] - 252s 252ms/step - loss: 0.0040 - mean q: 0.9715 - rewards: -0.0055 - reward sum: -1794.2498 - l/s: -0.0375 - t: 242.8413\n",
      "1000/1000 [==============================] - 251s 250ms/step - loss: 0.0038 - mean q: 0.9824 - rewards: -0.0115 - reward sum: -1840.2938 - l/s: -0.1205 - t: 251.8502\n",
      "1000/1000 [==============================] - 260s 260ms/step - loss: 0.0026 - mean q: 1.0094 - rewards: -0.0042 - reward sum: -1856.9678 - l/s: -0.0810 - t: 264.3723\n",
      "1000/1000 [==============================] - 248s 248ms/step - loss: 0.0032 - mean q: 1.0367 - rewards: 0.0191 - reward sum: -1780.7440 - l/s: -0.0665 - t: 244.8302\n",
      "1000/1000 [==============================] - 251s 251ms/step - loss: 0.0032 - mean q: 1.0649 - rewards: -0.0082 - reward sum: -1813.6217 - l/s: -0.1790 - t: 268.1963\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.0054 - mean q: 1.1139 - rewards: -0.0143 - reward sum: -1870.7691 - l/s: -0.1760 - t: 260.0395\n",
      "1000/1000 [==============================] - 264s 264ms/step - loss: 0.0021 - mean q: 1.1368 - rewards: 0.0024 - reward sum: -1861.0464 - l/s: -0.2079 - t: 257.5414\n",
      "1000/1000 [==============================] - 269s 269ms/step - loss: 0.0024 - mean q: 1.1640 - rewards: -4.5827e-04 - reward sum: -1862.8795 - l/s: -0.1800 - t: 265.4802\n",
      "1000/1000 [==============================] - 270s 270ms/step - loss: 0.0019 - mean q: 1.1949 - rewards: -0.0031 - reward sum: -1875.1412 - l/s: -0.2184 - t: 283.0133\n",
      "1000/1000 [==============================] - 269s 269ms/step - loss: 0.0022 - mean q: 1.2096 - rewards: -0.0125 - reward sum: -1925.0271 - l/s: -0.2439 - t: 277.3348\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 0.0027 - mean q: 1.2369 - rewards: -0.0184 - reward sum: -1998.6133 - l/s: -0.1755 - t: 326.9804\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.0019 - mean q: 1.2547 - rewards: -0.0159 - reward sum: -2062.2108 - l/s: -0.2019 - t: 285.5340\n",
      "1000/1000 [==============================] - 293s 293ms/step - loss: 0.0032 - mean q: 1.2826 - rewards: -0.0226 - reward sum: -2152.4229 - l/s: -0.0055 - t: 271.3401\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.0027 - mean q: 1.3165 - rewards: -0.0214 - reward sum: -2238.1916 - l/s: -0.1605 - t: 280.2466\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.0018 - mean q: 1.3354 - rewards: -0.0051 - reward sum: -2258.4105 - l/s: -0.0940 - t: 279.3050\n",
      "1000/1000 [==============================] - 290s 290ms/step - loss: 0.0020 - mean q: 1.3408 - rewards: -0.0182 - reward sum: -2331.2124 - l/s: -0.1545 - t: 296.5059\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.0029 - mean q: 1.3473 - rewards: -0.0037 - reward sum: -2346.0356 - l/s: -0.2039 - t: 276.0572\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 0.0032 - mean q: 1.3733 - rewards: 0.0160 - reward sum: -2282.0069 - l/s: -0.1705 - t: 323.5539\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.0023 - mean q: 1.4003 - rewards: -0.0226 - reward sum: -2372.4040 - l/s: -0.1820 - t: 285.5863\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.0022 - mean q: 1.4222 - rewards: -0.0149 - reward sum: -2431.8308 - l/s: -0.1735 - t: 283.8020\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.0020 - mean q: 1.4311 - rewards: -0.0121 - reward sum: -2480.0942 - l/s: -0.2554 - t: 323.7051\n",
      "1000/1000 [==============================] - 829s 829ms/step - loss: 0.0020 - mean q: 1.4532 - rewards: -0.0039 - reward sum: -2495.7459 - l/s: -0.1600 - t: 599.6328\n",
      "1000/1000 [==============================] - 2548s 3s/step - loss: 0.0030 - mean q: 1.4745 - rewards: -0.0235 - reward sum: -2589.6939 - l/s: -0.1600 - t: 469.2871\n",
      "1000/1000 [==============================] - 1189s 1s/step - loss: 0.0045 - mean q: 1.4864 - rewards: -0.0352 - reward sum: -2730.6824 - l/s: -0.1355 - t: 313.1704\n",
      "1000/1000 [==============================] - 382s 382ms/step - loss: 0.0073 - mean q: 1.4976 - rewards: -0.0472 - reward sum: -2919.3261 - l/s: -0.1445 - t: 311.9592\n",
      "1000/1000 [==============================] - 363s 363ms/step - loss: 0.0027 - mean q: 1.5041 - rewards: -0.0088 - reward sum: -2954.3290 - l/s: -0.1805 - t: 316.2506\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.0037 - mean q: 1.5138 - rewards: -0.0177 - reward sum: -3025.1396 - l/s: -0.1570 - t: 338.6778\n",
      "1000/1000 [==============================] - 326s 326ms/step - loss: 0.0017 - mean q: 1.5250 - rewards: -0.0108 - reward sum: -3068.3138 - l/s: -0.1330 - t: 410.7231\n",
      "1000/1000 [==============================] - 369s 369ms/step - loss: 0.0035 - mean q: 1.5263 - rewards: -0.0156 - reward sum: -3130.5356 - l/s: -0.1970 - t: 333.2094\n",
      "1000/1000 [==============================] - 335s 335ms/step - loss: 0.0020 - mean q: 1.5336 - rewards: -0.0122 - reward sum: -3179.3661 - l/s: -0.2044 - t: 352.3624\n",
      "1000/1000 [==============================] - 326s 326ms/step - loss: 0.0021 - mean q: 1.5355 - rewards: -0.0235 - reward sum: -3273.4898 - l/s: -0.1685 - t: 309.5718\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.0029 - mean q: 1.5452 - rewards: -0.0244 - reward sum: -3370.9707 - l/s: -0.1505 - t: 299.1366\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.0035 - mean q: 1.5560 - rewards: 0.0052 - reward sum: -3349.9785 - l/s: -0.1445 - t: 316.6698\n",
      "1000/1000 [==============================] - 319s 320ms/step - loss: 0.0035 - mean q: 1.5737 - rewards: 0.0059 - reward sum: -3326.4936 - l/s: -0.0730 - t: 290.4895\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.0037 - mean q: 1.5475 - rewards: -0.0117 - reward sum: -3373.3881 - l/s: -0.0530 - t: 292.8818\n",
      "1000/1000 [==============================] - 312s 312ms/step - loss: 0.0036 - mean q: 1.5539 - rewards: -0.0026 - reward sum: -3383.9803 - l/s: -0.1280 - t: 343.2183\n",
      "1000/1000 [==============================] - 313s 313ms/step - loss: 0.0038 - mean q: 1.5728 - rewards: -0.0308 - reward sum: -3507.0892 - l/s: -0.2004 - t: 289.8736\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.0037 - mean q: 1.5634 - rewards: 0.0091 - reward sum: -3470.5273 - l/s: -0.1750 - t: 292.9094\n",
      "1000/1000 [==============================] - 309s 309ms/step - loss: 0.0023 - mean q: 1.5751 - rewards: -0.0083 - reward sum: -3503.6817 - l/s: -0.1800 - t: 333.6766\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.0028 - mean q: 1.5904 - rewards: -0.0036 - reward sum: -3518.0395 - l/s: -0.1840 - t: 292.6752\n",
      "1000/1000 [==============================] - 350s 350ms/step - loss: 0.0032 - mean q: 1.6132 - rewards: -0.0293 - reward sum: -3635.0401 - l/s: -0.1200 - t: 330.2662\n",
      "1000/1000 [==============================] - 327s 327ms/step - loss: 0.0024 - mean q: 1.6256 - rewards: -0.0163 - reward sum: -3700.4173 - l/s: -0.1525 - t: 336.7410\n",
      "1000/1000 [==============================] - 313s 313ms/step - loss: 0.0020 - mean q: 1.6278 - rewards: -0.0210 - reward sum: -3784.5961 - l/s: -0.1940 - t: 305.1442\n",
      "1000/1000 [==============================] - 324s 324ms/step - loss: 0.0024 - mean q: 1.6350 - rewards: -0.0030 - reward sum: -3796.5078 - l/s: -0.1895 - t: 291.3382\n",
      "1000/1000 [==============================] - 311s 311ms/step - loss: 0.0025 - mean q: 1.6456 - rewards: -0.0465 - reward sum: -3982.6847 - l/s: -0.1645 - t: 305.6976\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.0026 - mean q: 1.6466 - rewards: 0.0217 - reward sum: -3895.8922 - l/s: -0.1600 - t: 307.9306\n",
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.0030 - mean q: 1.6496 - rewards: -0.0116 - reward sum: -3942.4536 - l/s: -0.1650 - t: 338.1371\n",
      "1000/1000 [==============================] - 340s 339ms/step - loss: 0.0020 - mean q: 1.6600 - rewards: 0.0040 - reward sum: -3926.6475 - l/s: -0.1825 - t: 308.6116\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.0025 - mean q: 1.6646 - rewards: -0.0257 - reward sum: -4029.4640 - l/s: -0.1855 - t: 315.7830\n",
      "1000/1000 [==============================] - 312s 312ms/step - loss: 0.0021 - mean q: 1.6766 - rewards: -0.0117 - reward sum: -4076.3886 - l/s: -0.2279 - t: 305.2261\n",
      "1000/1000 [==============================] - 328s 328ms/step - loss: 0.0021 - mean q: 1.6821 - rewards: 0.0110 - reward sum: -4032.4868 - l/s: -0.1785 - t: 306.3608\n",
      "1000/1000 [==============================] - 346s 346ms/step - loss: 0.0021 - mean q: 1.6941 - rewards: -0.0075 - reward sum: -4062.4189 - l/s: -0.2059 - t: 466.6158\n",
      "1000/1000 [==============================] - 491s 491ms/step - loss: 0.0022 - mean q: 1.7035 - rewards: -0.0070 - reward sum: -4090.2361 - l/s: -0.2019 - t: 524.6995\n",
      "1000/1000 [==============================] - 595s 595ms/step - loss: 0.0020 - mean q: 1.6971 - rewards: -0.0100 - reward sum: -4130.2626 - l/s: -0.2149 - t: 805.3045\n",
      "1000/1000 [==============================] - 655s 655ms/step - loss: 0.0054 - mean q: 1.7022 - rewards: 0.0061 - reward sum: -4105.8174 - l/s: -0.1865 - t: 968.5812\n",
      "1000/1000 [==============================] - 682s 681ms/step - loss: 0.0029 - mean q: 1.7050 - rewards: -0.0076 - reward sum: -4136.2166 - l/s: -0.0960 - t: 523.0102\n",
      "1000/1000 [==============================] - 710s 710ms/step - loss: 0.0024 - mean q: 1.6958 - rewards: -0.0134 - reward sum: -4189.6449 - l/s: -0.0740 - t: 587.6392\n",
      "1000/1000 [==============================] - 706s 706ms/step - loss: 0.0028 - mean q: 1.7003 - rewards: -0.0099 - reward sum: -4229.3701 - l/s: -0.0790 - t: 556.4195\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 0.0048 - mean q: 1.7076 - rewards: -0.0320 - reward sum: -4357.2988 - l/s: -0.0450 - t: 501.0114\n",
      "1000/1000 [==============================] - 722s 722ms/step - loss: 0.0025 - mean q: 1.7079 - rewards: -0.0118 - reward sum: -4404.5896 - l/s: -0.1030 - t: 737.8200\n",
      "1000/1000 [==============================] - 685s 685ms/step - loss: 0.0026 - mean q: 1.7047 - rewards: -0.0140 - reward sum: -4460.6355 - l/s: -0.0520 - t: 526.8844\n",
      "1000/1000 [==============================] - 673s 673ms/step - loss: 0.0032 - mean q: 1.7296 - rewards: -0.0248 - reward sum: -4559.7609 - l/s: -0.0815 - t: 679.3272\n",
      "1000/1000 [==============================] - 662s 662ms/step - loss: 0.0024 - mean q: 1.7522 - rewards: -0.0035 - reward sum: -4573.8499 - l/s: -0.1815 - t: 753.4106\n",
      "1000/1000 [==============================] - 696s 697ms/step - loss: 0.0021 - mean q: 1.7610 - rewards: -0.0112 - reward sum: -4618.7918 - l/s: -0.1100 - t: 704.7139\n",
      "1000/1000 [==============================] - 702s 703ms/step - loss: 0.0020 - mean q: 1.7685 - rewards: -0.0094 - reward sum: -4656.4248 - l/s: -0.0385 - t: 770.7129\n",
      "1000/1000 [==============================] - 699s 699ms/step - loss: 0.0026 - mean q: 1.7874 - rewards: -0.0131 - reward sum: -4708.7659 - l/s: -0.1060 - t: 445.5547\n",
      "1000/1000 [==============================] - 732s 733ms/step - loss: 0.0031 - mean q: 1.7942 - rewards: -0.0236 - reward sum: -4803.1375 - l/s: -0.1485 - t: 879.8425\n",
      "1000/1000 [==============================] - 732s 733ms/step - loss: 0.0016 - mean q: 1.8104 - rewards: -0.0105 - reward sum: -4845.1923 - l/s: -0.1930 - t: 794.9339\n",
      "1000/1000 [==============================] - 727s 727ms/step - loss: 0.0019 - mean q: 1.8209 - rewards: -0.0125 - reward sum: -4895.0465 - l/s: -0.1190 - t: 820.6812\n",
      "1000/1000 [==============================] - 738s 738ms/step - loss: 0.0019 - mean q: 1.8262 - rewards: -0.0054 - reward sum: -4916.6509 - l/s: -0.2039 - t: 557.3657\n",
      "1000/1000 [==============================] - 735s 736ms/step - loss: 0.0019 - mean q: 1.8296 - rewards: -0.0103 - reward sum: -4957.9189 - l/s: -0.1250 - t: 746.7664\n",
      "1000/1000 [==============================] - 747s 747ms/step - loss: 0.0037 - mean q: 1.8493 - rewards: -0.0082 - reward sum: -4990.7342 - l/s: -0.2209 - t: 700.0757\n",
      "1000/1000 [==============================] - 762s 762ms/step - loss: 0.0026 - mean q: 1.8831 - rewards: -0.0230 - reward sum: -5082.6949 - l/s: -0.2419 - t: 726.1014\n",
      "1000/1000 [==============================] - 798s 798ms/step - loss: 0.0026 - mean q: 1.8939 - rewards: -0.0175 - reward sum: -5152.5613 - l/s: -0.1955 - t: 636.1221\n",
      "1000/1000 [==============================] - 779s 779ms/step - loss: 0.0020 - mean q: 1.9104 - rewards: -0.0216 - reward sum: -5238.9148 - l/s: -0.1860 - t: 980.0142\n",
      "1000/1000 [==============================] - 791s 792ms/step - loss: 0.0025 - mean q: 1.9450 - rewards: -0.0055 - reward sum: -5260.8295 - l/s: -0.1505 - t: 524.5297\n",
      "1000/1000 [==============================] - 856s 856ms/step - loss: 0.0021 - mean q: 1.9550 - rewards: -0.0134 - reward sum: -5314.3110 - l/s: -0.1235 - t: 967.7449\n",
      "1000/1000 [==============================] - 835s 835ms/step - loss: 0.0025 - mean q: 1.9857 - rewards: -0.0211 - reward sum: -5398.8536 - l/s: -0.1780 - t: 922.0169\n",
      "1000/1000 [==============================] - 815s 815ms/step - loss: 0.0014 - mean q: 2.0090 - rewards: -0.0233 - reward sum: -5492.0575 - l/s: -0.1715 - t: 779.3107\n",
      "1000/1000 [==============================] - 814s 815ms/step - loss: 0.0014 - mean q: 2.0241 - rewards: -0.0202 - reward sum: -5572.7323 - l/s: -0.1630 - t: 928.9791\n",
      "1000/1000 [==============================] - 819s 819ms/step - loss: 0.0020 - mean q: 1.9967 - rewards: -0.0394 - reward sum: -5730.2002 - l/s: -0.0600 - t: 764.8971\n",
      "1000/1000 [==============================] - 810s 810ms/step - loss: 0.0021 - mean q: 1.9872 - rewards: -0.0375 - reward sum: -5880.0390 - l/s: -0.1545 - t: 699.0292\n",
      "1000/1000 [==============================] - 808s 808ms/step - loss: 0.0019 - mean q: 1.9733 - rewards: -0.0055 - reward sum: -5902.0662 - l/s: -0.1910 - t: 742.7778\n",
      "1000/1000 [==============================] - 818s 818ms/step - loss: 0.0018 - mean q: 1.9606 - rewards: 8.6505e-05 - reward sum: -5901.7202 - l/s: -0.1440 - t: 824.0026\n",
      "1000/1000 [==============================] - 846s 847ms/step - loss: 0.0012 - mean q: 1.9614 - rewards: -0.0346 - reward sum: -6040.1313 - l/s: -0.0895 - t: 763.7264\n",
      "1000/1000 [==============================] - 855s 855ms/step - loss: 0.0025 - mean q: 1.9556 - rewards: -0.0237 - reward sum: -6134.9885 - l/s: -0.0545 - t: 852.0358\n",
      "1000/1000 [==============================] - 816s 817ms/step - loss: 0.0017 - mean q: 1.9572 - rewards: -0.0074 - reward sum: -6164.4721 - l/s: -0.0940 - t: 826.2913\n",
      "1000/1000 [==============================] - 848s 848ms/step - loss: 0.0020 - mean q: 1.9631 - rewards: -0.0177 - reward sum: -6235.2726 - l/s: -0.1215 - t: 676.4910\n",
      "1000/1000 [==============================] - 886s 886ms/step - loss: 0.0014 - mean q: 1.9511 - rewards: -0.0117 - reward sum: -6282.0848 - l/s: -0.0225 - t: 704.2982\n",
      "1000/1000 [==============================] - 858s 858ms/step - loss: 0.0019 - mean q: 1.9528 - rewards: -0.0095 - reward sum: -6319.9688 - l/s: -0.0620 - t: 885.1562\n",
      "1000/1000 [==============================] - 841s 841ms/step - loss: 0.0025 - mean q: 1.9552 - rewards: -0.0110 - reward sum: -6363.8735 - l/s: -0.1840 - t: 955.0138\n",
      "1000/1000 [==============================] - 857s 858ms/step - loss: 0.0021 - mean q: 1.9911 - rewards: -0.0223 - reward sum: -6453.0548 - l/s: -0.1640 - t: 973.3410\n",
      "1000/1000 [==============================] - 840s 840ms/step - loss: 0.0025 - mean q: 1.9899 - rewards: -0.0393 - reward sum: -6610.1681 - l/s: -0.1540 - t: 770.1171\n",
      "1000/1000 [==============================] - 867s 867ms/step - loss: 0.0043 - mean q: 2.0055 - rewards: -0.0096 - reward sum: -6648.7150 - l/s: -0.0835 - t: 1061.3536\n",
      "1000/1000 [==============================] - 867s 867ms/step - loss: 0.0022 - mean q: 2.0252 - rewards: 0.0081 - reward sum: -6616.2805 - l/s: -0.1225 - t: 933.1946\n",
      "1000/1000 [==============================] - 901s 901ms/step - loss: 0.0025 - mean q: 2.0384 - rewards: -0.0078 - reward sum: -6647.5107 - l/s: -0.0365 - t: 1215.0534\n",
      "1000/1000 [==============================] - 597s 596ms/step - loss: 0.0035 - mean q: 2.0380 - rewards: -0.0214 - reward sum: -6732.9117 - l/s: -0.1785 - t: 358.6354\n",
      "1000/1000 [==============================] - 358s 358ms/step - loss: 0.0022 - mean q: 2.0531 - rewards: 0.0172 - reward sum: -6663.9408 - l/s: -0.1605 - t: 357.0726\n",
      "1000/1000 [==============================] - 354s 355ms/step - loss: 0.0037 - mean q: 2.0653 - rewards: -0.0031 - reward sum: -6676.2902 - l/s: -0.1820 - t: 364.0867\n",
      "1000/1000 [==============================] - 352s 353ms/step - loss: 0.0029 - mean q: 2.0861 - rewards: 1.1544e-04 - reward sum: -6675.8285 - l/s: -0.2059 - t: 394.5609\n",
      "1000/1000 [==============================] - 351s 351ms/step - loss: 0.0056 - mean q: 2.0569 - rewards: 0.0037 - reward sum: -6660.9161 - l/s: -0.2319 - t: 363.4310\n",
      "1000/1000 [==============================] - 355s 355ms/step - loss: 0.0025 - mean q: 2.0723 - rewards: -0.0155 - reward sum: -6722.9535 - l/s: -0.1445 - t: 380.6626\n",
      "1000/1000 [==============================] - 353s 353ms/step - loss: 0.0029 - mean q: 2.0994 - rewards: -0.0139 - reward sum: -6778.5314 - l/s: -0.1850 - t: 378.9845\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 0.0031 - mean q: 2.1151 - rewards: 2.9407e-05 - reward sum: -6778.4137 - l/s: -0.1925 - t: 371.2699\n",
      "1000/1000 [==============================] - 352s 352ms/step - loss: 0.0023 - mean q: 2.1275 - rewards: -0.0235 - reward sum: -6872.2259 - l/s: -0.1970 - t: 355.5823\n",
      "1000/1000 [==============================] - 349s 349ms/step - loss: 0.0027 - mean q: 2.1417 - rewards: -0.0119 - reward sum: -6919.8416 - l/s: -0.1400 - t: 398.6770\n",
      "1000/1000 [==============================] - 350s 350ms/step - loss: 0.0025 - mean q: 2.1374 - rewards: -0.0031 - reward sum: -6932.4265 - l/s: -0.1065 - t: 395.6099\n",
      "1000/1000 [==============================] - 353s 353ms/step - loss: 0.0034 - mean q: 2.1544 - rewards: -0.0126 - reward sum: -6982.8572 - l/s: -0.1590 - t: 342.5355\n",
      "1000/1000 [==============================] - 477s 477ms/step - loss: 0.0026 - mean q: 2.1859 - rewards: -0.0338 - reward sum: -7117.9429 - l/s: -0.2124 - t: 868.8069\n",
      " 881/1000 [=========================>....] - ETA: 5:17 - loss: 0.0027 - mean q: 2.2055 - rewards: -0.0167 - reward sum: -7176.8252 - l/s: -0.2156 - t: 423.6396"
     ]
    }
   ],
   "source": [
    "\n",
    "num_parallel = training_parallel\n",
    "envs = [environment() for _ in range(num_parallel)]\n",
    "\n",
    "\n",
    "print(\"training...\")\n",
    "\n",
    "n = 100000000\n",
    "agent.train(num_steps = n, envs = envs, warmup = 0, log_interval = 1000, train_steps_per_step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cf8bb-22fe-41af-8ed1-428df91e39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b9a35-1045-4c99-bc9f-6708ba0f7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316311f-cb67-498e-9172-1dcedfcfb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
