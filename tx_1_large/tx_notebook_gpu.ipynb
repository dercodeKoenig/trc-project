{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb26e65-1762-47db-8a66-e80da35bbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "import ta\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "name = \"dqn_trading_transformer\"\n",
    "log_folder = \"./\"\n",
    "candles_dir = \"../candles/\"\n",
    "\n",
    "training_parallel = 4\n",
    "warmup_parallel = 8\n",
    "warmup_steps = 5000\n",
    "\n",
    "\n",
    "loss_prio_sample_mult = 32\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "memory_size = 300000\n",
    "lr  = 0.0002\n",
    "seq_len = 600\n",
    "\n",
    "soft_reward_inc = 1.05\n",
    "comission = 10/100000\n",
    "\n",
    "#resume = True\n",
    "resume = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433ff867-691a-4d63-8647-a4dcc04f95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_to_x(sample):\n",
    "        \n",
    "                current_close = sample[-1].c\n",
    "\n",
    "                prev_close = [candle.c for candle in sample]\n",
    "                prev_high = [candle.h for candle in sample]\n",
    "                prev_low = [candle.l for candle in sample]\n",
    "                \n",
    "\n",
    "                prev_sma21 = [candle.sma21 for candle in sample]\n",
    "                prev_sma200 = [candle.sma200 for candle in sample]\n",
    "                \n",
    "                \n",
    "                prev_sma21_relative = [(prev_close[o] - prev_sma21[o]) / prev_sma21[o]*100 for o in range(seq_len)]\n",
    "                prev_sma200_relative = [(prev_close[o] - prev_sma200[o]) / prev_sma200[o]*100 for o in range(seq_len)]\n",
    "\n",
    "                prev_close_relative = [0] + [(prev_close[o+1] - prev_close[o]) / prev_close[o]*1000 for o in range(seq_len-1)]\n",
    "                prev_high_relative = [(prev_close[o] - prev_high[o]) / prev_close[o]*1000 for o in range(seq_len)]\n",
    "                prev_low_relative = [(prev_close[o] - prev_low[o]) / prev_close[o]*1000 for o in range(seq_len)]\n",
    "                \n",
    "\n",
    "                \n",
    "                prev_rsi_14 = [candle.rsi14 for candle in sample]\n",
    "                \n",
    "\n",
    "                x = []\n",
    "                for o in range(len(prev_close)):\n",
    "                    ts = []\n",
    "\n",
    "                    \n",
    "                    ts.append(prev_close_relative[o])\n",
    "                    ts.append(prev_high_relative[o])\n",
    "                    ts.append(prev_low_relative[o])\n",
    "                    \n",
    "                    ts.append(prev_sma21_relative[o])\n",
    "                    ts.append(prev_sma200_relative[o])\n",
    "                    \n",
    "                    ts.append(prev_rsi_14[o])\n",
    "\n",
    "                    x.append(ts)\n",
    "\n",
    "                x = np.array(x)\n",
    "                return x\n",
    "        \n",
    "\n",
    "\n",
    "def Load(file):\n",
    "    f = open(file, \"rb\")\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log_interval = 4*24 # environment logs daily returns\n",
    "\n",
    "class candle_class:\n",
    "    pass\n",
    "  \n",
    "order_value = 1000\n",
    "\n",
    "\n",
    "class environment():\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def _next_observation(self):\n",
    "            candles = self.candles[self.current_step - seq_len + 1:self.current_step + 1]\n",
    "            \n",
    "            inference_data = sample_to_x(candles)\n",
    "            \n",
    "            return inference_data, np.array([self.position, math.tanh(self.current_win)])\n",
    "\n",
    "  \n",
    "  def reset(self, first_reset = False):\n",
    "    self.candles = None\n",
    "    candles_files = os.listdir(candles_dir)\n",
    "    use_file = candles_dir+random.choice(candles_files)\n",
    "    #print(use_file)\n",
    "    self.candles = Load(use_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.current_step = 200+seq_len if first_reset == False else random.randint(200+seq_len,len(self.candles) - 1000)\n",
    "    self.position = 0\n",
    "    self.entry_price = 0\n",
    "    self.win = 0\n",
    "    self.current_win = 0\n",
    "    self.startindex = self.current_step\n",
    "    self.last_reward = 0\n",
    "    self.reward_tr_given = 0\n",
    "    self.reward_since_last_log = 0\n",
    "    self.closed_trades_since_last_log = 0\n",
    "\n",
    "    return self._next_observation()\n",
    "\n",
    "  \n",
    "  def close(self):\n",
    "        self.win -= comission * order_value / 2\n",
    "        self.position = 0\n",
    "        self.win+=self.current_win - self.reward_tr_given\n",
    "        self.reward_tr_given = 0\n",
    "        self.current_win = 0\n",
    "        self.closed_trades_since_last_log+=1\n",
    "        \n",
    "        \n",
    "  def entry(self):\n",
    "        self.entry_price = self.candles[self.current_step].c\n",
    "        self.win -= comission * order_value / 2\n",
    "\n",
    "  def step(self, action):\n",
    "    action+=1 # disable no position\n",
    "    \n",
    "    if action == 0:\n",
    "        if self.position != 0:\n",
    "            self.close()\n",
    "    \n",
    "    if action == 1:\n",
    "      #short\n",
    "      if self.position == 1:\n",
    "        self.close()\n",
    "\n",
    "      if self.position == -1:\n",
    "        pass\n",
    "      else:\n",
    "        self.position = -1\n",
    "        self.entry()\n",
    "        \n",
    "    if action == 2:\n",
    "      #long\n",
    "      if self.position == -1:\n",
    "        self.close()\n",
    "\n",
    "      if self.position == 1:\n",
    "        pass\n",
    "      else:\n",
    "        self.position = 1\n",
    "        self.entry()\n",
    "        \n",
    "    self.current_step += 1\n",
    "    if self.position != 0:\n",
    "      current_price = self.candles[self.current_step].c\n",
    "      entry = self.entry_price\n",
    "      diff = (current_price - entry) / entry * order_value\n",
    "\n",
    "      if self.position == 1:\n",
    "        self.current_win = diff\n",
    "      if self.position == -1:\n",
    "        self.current_win = -diff\n",
    "\n",
    "        \n",
    "    diff = self.current_win - self.reward_tr_given\n",
    "    reward_inc = diff / soft_reward_inc\n",
    "    self.reward_tr_given += reward_inc\n",
    "    self.win += reward_inc\n",
    "    \n",
    "    reward_raw = self.win# + self.current_win  # sparse reward disabled\n",
    "    reward = reward_raw - self.last_reward\n",
    "    self.last_reward = reward_raw\n",
    "    reward = max(min(reward, 10), -10)\n",
    "    \n",
    "    \n",
    "    done = self.current_step == len(self.candles) -1\n",
    "    \n",
    "    if (self.current_step - self.startindex) % log_interval == 0:\n",
    "        log_reward = reward_raw - self.reward_since_last_log\n",
    "        log_reward = max(min(log_reward, 200), -200)\n",
    "        self.reward_since_last_log = reward_raw \n",
    "        file2 = open(log_folder+\"logs/r2_log.txt\", \"a\")  \n",
    "        file2.write(str(log_reward))\n",
    "        file2.write(\"\\n\")\n",
    "        file2.close()\n",
    "        \n",
    "        \n",
    "        file2 = open(log_folder+\"logs/num_trades_per_day.txt\", \"a\")  \n",
    "        file2.write(str(self.closed_trades_since_last_log))\n",
    "        file2.write(\"\\n\")\n",
    "        file2.close()\n",
    "        \n",
    "        \n",
    "        self.closed_trades_since_last_log = 0\n",
    "    \n",
    "    obs = self._next_observation()\n",
    "    return obs, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc54f552-1e9d-42a7-9877-12297ea740d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 600, 6)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 600, 64)      1216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600, 70)      0           conv1d[0][0]                     \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 600, 32)      2272        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 600, 32)      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 600, 32)      64          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 600, 128)     20608       layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600, 160)     0           conv1d_1[0][0]                   \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 600, 64)      10304       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 600, 64)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 600, 64)      128         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 600, 128)     41088       layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600, 192)     0           conv1d_2[0][0]                   \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 600, 192)     384         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 600, 128)     24704       layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 600, 128)     82048       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 600, 128)     0           conv1d_3[0][0]                   \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 600, 128)     82048       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 600, 128)     0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 600, 128)     82048       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 600, 128)     0           conv1d_5[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 600, 128)     82048       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 600, 128)     0           conv1d_6[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 600, 128)     82048       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 600, 128)     0           conv1d_7[0][0]                   \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 600, 128)     82048       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 600, 128)     0           conv1d_8[0][0]                   \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 600, 128)     82048       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 600, 128)     0           conv1d_9[0][0]                   \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 600, 32)      4128        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 600, 32)      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 600, 16)      528         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 600, 16)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 6)            0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 9600)         0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 6)            0           tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 9608)         0           input_2[0][0]                    \n",
      "                                                                 flatten[0][0]                    \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         9839616     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1049600     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1049600     leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2)            2048        leaky_re_lu_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 13,670,224\n",
      "Trainable params: 13,670,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, model,\n",
    "                 n_actions,\n",
    "                 memory_size = 10000, \n",
    "                 optimizer = tf.keras.optimizers.Adam(0.0005), \n",
    "                 gamma = 0.99,\n",
    "                 batch_size =32,\n",
    "                 name = \"dqn1\",\n",
    "                 target_model_sync = 1000,\n",
    "                 exploration = 0.01\n",
    "                ):\n",
    "        self.exploration = exploration\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.memory_size = memory_size\n",
    "        self.optimizer = optimizer\n",
    "        self.m1 = np.eye(self.n_actions, dtype=\"float32\")\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_sync = target_model_sync\n",
    "   \n",
    "        self.memory = deque(maxlen = self.memory_size)\n",
    "      \n",
    "    \n",
    "    def copy_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "      \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.name)\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.name, overwrite = True)\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def model_call(self, x):\n",
    "        x1, x2 = x\n",
    "        return tf.math.argmax(self.model([x1,x2]), axis = 1)\n",
    "    \n",
    "    def select_actions(self, current_states, positions):\n",
    "        \n",
    "        ret = self.model_call([current_states, positions])\n",
    "        return ret\n",
    "\n",
    "\n",
    "        \n",
    "    def observe_sasrt(self, state, action, next_state, reward, terminal):\n",
    "        self.memory.append([state, action, reward, 1-int(terminal), next_state, 1])\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def get_target_q(self, next_states, rewards, terminals):\n",
    "        estimated_q_values_next = self.target_model(next_states)\n",
    "        q_batch = tf.math.reduce_max(estimated_q_values_next, axis=1)\n",
    "        target_q_values = q_batch * self.gamma * terminals + rewards\n",
    "        return target_q_values\n",
    "\n",
    "        \n",
    "    @tf.function(jit_compile = False)\n",
    "    def tstep(self, data):\n",
    "        states, next_states, rewards, terminals, masks, idx = data\n",
    "        target_q_values = self.get_target_q(next_states, rewards, terminals)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            estimated_q_values = tf.math.reduce_sum(self.model(states, training=True) * masks, axis=1)\n",
    "            loss_e = tf.math.square(target_q_values - estimated_q_values)\n",
    "            loss = tf.reduce_mean(loss_e)\n",
    "        \n",
    "        \n",
    "        gradient = t.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "        \n",
    "        return loss, tf.reduce_mean(estimated_q_values), loss_e, idx\n",
    "    \n",
    "    \n",
    "    def data_get_func(self):\n",
    "        idx = np.random.randint(0, len(self.memory), self.batch_size*loss_prio_sample_mult)\n",
    "        for i in idx:\n",
    "            self.memory[i][-1]*=1.001\n",
    "        sarts_batch = [self.memory[i] for i in idx]\n",
    "        sorted_batch_idx = sorted(zip(sarts_batch, idx), key=lambda x:x[0][-1])[0:self.batch_size]\n",
    "        sarts_batch = [i[0] for i in sorted_batch_idx]\n",
    "        sorted_idx = [i[1] for i in sorted_batch_idx]\n",
    "        \n",
    "        states = [x[0] for x in sarts_batch]\n",
    "        states_1 = np.array([x[0] for x in states], dtype=\"float32\")\n",
    "        states_2 = np.array([x[1] for x in states], dtype=\"float32\")\n",
    "        \n",
    "        actions = [x[1] for x in sarts_batch]\n",
    "        rewards = np.array([x[2] for x in sarts_batch], dtype=\"float32\")\n",
    "        terminals = np.array([x[3] for x in sarts_batch], dtype=\"float32\")\n",
    "        \n",
    "        next_states = [x[4] for x in sarts_batch]\n",
    "        next_states_1 = np.array([x[0] for x in next_states], dtype=\"float32\")\n",
    "        next_states_2 = np.array([x[1] for x in next_states], dtype=\"float32\")\n",
    "        \n",
    "        masks = self.m1[actions]\n",
    "        return [states_1, states_2], [next_states_1, next_states_2], rewards, terminals, masks, sorted_idx\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.total_steps_trained+=1\n",
    "        if self.total_steps_trained % self.target_model_sync == 0:\n",
    "            self.copy_weights()\n",
    "\n",
    "           \n",
    "        data = self.data_get_func()\n",
    "        result= self.tstep(data)\n",
    "    \n",
    "        map_loss = [x.numpy() for x in result[2]]\n",
    "        map_idx =[x.numpy() for x in result[3]]\n",
    "#        print(map_loss, map_idx)\n",
    "\n",
    "        for l, i in zip(map_loss,map_idx):\n",
    "                self.memory[i][-1] = l\n",
    "\n",
    "        \n",
    "        return  result[0:2]\n",
    "    \n",
    "    \n",
    "    def train(self, num_steps, envs, log_interval = 1000, warmup = 0, train_steps_per_step = 1):\n",
    "        self.total_steps_trained = -1\n",
    "\n",
    "        num_envs = len(envs)\n",
    "        states = [x.reset(True) for x in envs]\n",
    "        \n",
    "        current_episode_reward_sum = 0\n",
    "        times= deque(maxlen=10)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.longs = 0\n",
    "        self.shorts = 0\n",
    "\n",
    "        self.total_rewards = []\n",
    "        self.losses = [0]\n",
    "        self.q_v = [0]\n",
    "        \n",
    "        def save_current_run():\n",
    "            self.save_weights()\n",
    "            file = open(log_folder+\"logs/loss_log.txt\", \"a\")  \n",
    "            #for loss in self.losses:\n",
    "                        #file.write(str(loss))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.losses)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/qv_log.txt\", \"a\")  \n",
    "            #for qv in self.q_v:\n",
    "                        #file.write(str(qv))\n",
    "                        #file.write(\"\\n\")\n",
    "            file.write(str(np.mean(self.q_v)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "\n",
    "            file = open(log_folder+\"logs/rewards_log.txt\", \"a\")  \n",
    "            #for total_reward in self.total_rewards:\n",
    "                        #file.write(str(total_reward))\n",
    "                        #file.write(\"\\n\")\n",
    "                    \n",
    "            file.write(str(np.mean(self.total_rewards)))\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "            \n",
    "    \n",
    "\n",
    "            self.total_rewards = []\n",
    "            self.losses = [0]\n",
    "            self.q_v = [0]\n",
    "        \n",
    "        try:\n",
    "            for i in range(num_steps):\n",
    "                if i % log_interval == 0:\n",
    "                    progbar = tf.keras.utils.Progbar(log_interval, interval=0.05, stateful_metrics = [\"reward sum\", \"t\", \"l/s\"])\n",
    "                    self.longs = 0\n",
    "                    self.shorts = 0\n",
    "\n",
    "\n",
    "                states_1 = np.array([x[0] for x in states])\n",
    "                states_2 = np.array([x[1] for x in states])\n",
    "                actions = self.select_actions(states_1, states_2)\n",
    "                for action in actions:\n",
    "                    if action == 0:\n",
    "                        self.shorts+=1\n",
    "                    elif action == 1:\n",
    "                        self.longs+=1\n",
    "\n",
    "                sasrt_pairs = []\n",
    "                for index in range(num_envs):\n",
    "                    sasrt_pairs.append([states[index], actions[index]]+[x for x in envs[index].step(actions[index])])\n",
    "\n",
    "                next_states = [x[2] for x in sasrt_pairs]\n",
    "\n",
    "                reward = [x[3] for x in sasrt_pairs]\n",
    "                current_episode_reward_sum += np.sum(reward)\n",
    "\n",
    "                self.total_rewards.extend(reward)\n",
    "\n",
    "                for index, o in enumerate(sasrt_pairs):\n",
    "                    #print(o)\n",
    "                    if o[4] == True:\n",
    "                        next_states[index] = envs[index].reset()\n",
    "                    self.observe_sasrt(o[0], o[1], o[2], o[3], o[4])\n",
    "\n",
    "                states = next_states\n",
    "                if i > warmup:\n",
    "                    for _ in range(train_steps_per_step):\n",
    "                        loss, q = self.update_parameters()\n",
    "                        self.losses.append(loss.numpy())\n",
    "                        self.q_v.append(q.numpy())\n",
    "                else:\n",
    "                    loss, q = 0, 0\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed = (end_time - start_time) * 1000\n",
    "                times.append(elapsed)\n",
    "                start_time = end_time\n",
    "\n",
    "\n",
    "                if (i+1) % log_interval == 0:\n",
    "                    #print(\"-----------\")\n",
    "                    #print(\"l:\", np.mean(self.losses))\n",
    "                    #print(\"q:\", np.mean(self.q_v))\n",
    "                    #print(\"reward sum\", current_episode_reward_sum)\n",
    "                    #print(\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts))\n",
    "                    #print(\"t\", np.mean(times))\n",
    "                    #print(\"-----------\")\n",
    "                    save_current_run()\n",
    "\n",
    "                progbar.update(i%log_interval+1, values = \n",
    "                               [(\"loss\", np.mean(self.losses[-train_steps_per_step:])),\n",
    "                                (\"mean q\", np.mean(self.q_v[-train_steps_per_step:])),\n",
    "                                (\"rewards\", np.mean(reward)),\n",
    "                                (\"reward sum\", current_episode_reward_sum),\n",
    "                                (\"l/s\", (self.longs - self.shorts) / (1+self.longs+self.shorts)),\n",
    "                                (\"t\", np.mean(times))])\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nbreak!\")\n",
    "        \n",
    "        save_current_run()\n",
    "            \n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "if True:\n",
    "  inputs_1 = tf.keras.layers.Input(shape = (seq_len, 6))\n",
    "  inputs_pos = tf.keras.layers.Input(shape = (2))\n",
    "\n",
    "  x = inputs_1\n",
    "\n",
    "  x2 = tf.keras.layers.Conv1D(64, 3,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Concatenate()([x2,x])\n",
    "\n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Concatenate()([x2,x])\n",
    "\n",
    "  x = tf.keras.layers.Dense(64)(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Concatenate()([x2,x])\n",
    "  x = tf.keras.layers.LayerNormalization()(x)\n",
    "    \n",
    "  x = tf.keras.layers.Dense(128,activation = \"relu\")(x) \n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  x2 = tf.keras.layers.Conv1D(128, 5,activation=\"relu\", padding=\"same\")(x)\n",
    "  x = tf.keras.layers.Add()([x2,x])\n",
    "  \n",
    "  x = tf.keras.layers.Dense(32)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "  x = tf.keras.layers.Dense(16)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "  last_candle = tf.keras.layers.Reshape((6,))(inputs_1[:,-1])\n",
    "\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  \n",
    "  \n",
    "  x = tf.keras.layers.Concatenate()([inputs_pos, x, last_candle])\n",
    "  \n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "  x = tf.keras.layers.Dense(1024)(x)\n",
    "  x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "          \n",
    "  outputs = tf.keras.layers.Dense(2, activation = \"linear\", use_bias=False, dtype=\"float32\")(x)\n",
    "  model = tf.keras.Model([inputs_1,inputs_pos], outputs)\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f308778d-9f78-4241-837d-4cc0a20eb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr, clipvalue = 0.2)\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model = model, \n",
    "    n_actions = 2, \n",
    "    memory_size = memory_size, \n",
    "    gamma=gamma,\n",
    "    optimizer = opt,\n",
    "    batch_size = batch_size, \n",
    "    target_model_sync = 500,\n",
    "    exploration = 0.02,\n",
    "    name=log_folder+name+\".h5\")\n",
    "\n",
    "if resume:\n",
    "\tprint(\"loading weights...\")\n",
    "\tagent.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a6d1ee-e370-411c-99a8-9309c5acc113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup...\n",
      "5000/5000 [==============================] - 378s 74ms/step - loss: 0.0000e+00 - mean q: 0.0000e+00 - rewards: 4.3080e-04 - reward sum: 17.2321 - l/s: -0.9971 - t: 73.5818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\root\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\root\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_parallel = warmup_parallel\n",
    "envs = [environment() for _ in range(num_parallel)]\n",
    "\n",
    "print(\"warmup...\")\n",
    "\n",
    "n = int(warmup_steps)\n",
    "#n = int(100)\n",
    "agent.train(num_steps = n, envs = envs, warmup = n, log_interval = n, train_steps_per_step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1fdbe-6e3a-45b3-821c-03a10e6ae936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.4101 - mean q: -1.2764 - rewards: -0.0468 - reward sum: -187.1419 - l/s: 0.0910 - t: 235.5175\n",
      "1000/1000 [==============================] - 272s 272ms/step - loss: 0.0043 - mean q: -1.0158 - rewards: -0.0416 - reward sum: -353.3998 - l/s: -0.1220 - t: 270.8445\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.0041 - mean q: -0.8478 - rewards: -0.0235 - reward sum: -447.3947 - l/s: -0.2049 - t: 320.2672\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.0025 - mean q: -0.7345 - rewards: -0.0329 - reward sum: -578.9861 - l/s: -0.2254 - t: 310.2176\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.0038 - mean q: -0.6190 - rewards: -0.0292 - reward sum: -695.9796 - l/s: -0.1685 - t: 263.3743\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.0046 - mean q: -0.5360 - rewards: -0.0212 - reward sum: -780.7464 - l/s: -0.1930 - t: 309.5750\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.0051 - mean q: -0.4586 - rewards: -0.0275 - reward sum: -890.9311 - l/s: -0.1025 - t: 273.5582\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.0043 - mean q: -0.3904 - rewards: -0.0502 - reward sum: -1091.5678 - l/s: -0.0340 - t: 358.6080\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.0030 - mean q: -0.3185 - rewards: -0.0484 - reward sum: -1285.0933 - l/s: -0.1150 - t: 289.3359\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.0040 - mean q: -0.2640 - rewards: -0.0292 - reward sum: -1402.0782 - l/s: -0.1160 - t: 282.2298\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.0045 - mean q: -0.2235 - rewards: -0.0497 - reward sum: -1600.8846 - l/s: -0.0155 - t: 282.5522\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.0040 - mean q: -0.1909 - rewards: -0.0249 - reward sum: -1700.4419 - l/s: -0.0890 - t: 298.7221\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.0035 - mean q: -0.1395 - rewards: -0.0180 - reward sum: -1772.4851 - l/s: -0.1595 - t: 272.8442\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.0034 - mean q: -0.1010 - rewards: -0.0393 - reward sum: -1929.5855 - l/s: -0.0230 - t: 292.6165\n",
      "1000/1000 [==============================] - 295s 295ms/step - loss: 0.0035 - mean q: -0.0597 - rewards: -0.0270 - reward sum: -2037.5416 - l/s: -0.1010 - t: 278.5016\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.0032 - mean q: -2.6319e-04 - rewards: -0.0311 - reward sum: -2162.1224 - l/s: -0.0140 - t: 294.0145\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.0027 - mean q: 0.0600 - rewards: -0.0129 - reward sum: -2213.7080 - l/s: -0.0660 - t: 310.2638\n",
      "1000/1000 [==============================] - 301s 302ms/step - loss: 0.0028 - mean q: 0.1245 - rewards: -0.0299 - reward sum: -2333.4437 - l/s: -0.0185 - t: 280.0307\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.0032 - mean q: 0.1953 - rewards: -0.0238 - reward sum: -2428.5963 - l/s: -0.0665 - t: 301.8680\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.0027 - mean q: 0.2335 - rewards: -0.0202 - reward sum: -2509.3708 - l/s: -0.0800 - t: 252.4049\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.0030 - mean q: 0.2811 - rewards: -0.0233 - reward sum: -2602.6411 - l/s: -0.0905 - t: 290.8469\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.0028 - mean q: 0.3306 - rewards: -0.0383 - reward sum: -2755.8491 - l/s: 0.0240 - t: 316.4778\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.0024 - mean q: 0.3723 - rewards: -0.0247 - reward sum: -2854.5688 - l/s: 0.0595 - t: 288.4654\n",
      "1000/1000 [==============================] - 307s 307ms/step - loss: 0.0024 - mean q: 0.4059 - rewards: -0.0212 - reward sum: -2939.5382 - l/s: -0.0780 - t: 336.7881\n",
      "1000/1000 [==============================] - 332s 332ms/step - loss: 0.0026 - mean q: 0.4419 - rewards: -0.0241 - reward sum: -3036.0948 - l/s: -0.0600 - t: 462.3807\n",
      "1000/1000 [==============================] - 347s 347ms/step - loss: 0.0033 - mean q: 0.4858 - rewards: -0.0288 - reward sum: -3151.1069 - l/s: 0.0150 - t: 287.1065\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.0053 - mean q: 0.5392 - rewards: -0.0322 - reward sum: -3279.8130 - l/s: -0.0900 - t: 284.4846\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.0032 - mean q: 0.5688 - rewards: -0.0141 - reward sum: -3336.2347 - l/s: -0.0955 - t: 278.4513\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.0031 - mean q: 0.6046 - rewards: -0.0094 - reward sum: -3373.7495 - l/s: -0.0540 - t: 283.4972\n",
      "1000/1000 [==============================] - 306s 306ms/step - loss: 0.0019 - mean q: 0.6113 - rewards: -0.0094 - reward sum: -3411.2250 - l/s: -0.0685 - t: 315.9901\n",
      "1000/1000 [==============================] - 318s 319ms/step - loss: 0.0024 - mean q: 0.6491 - rewards: -0.0199 - reward sum: -3490.9318 - l/s: -0.0755 - t: 342.5451\n",
      "1000/1000 [==============================] - 318s 318ms/step - loss: 0.0025 - mean q: 0.6932 - rewards: -0.0218 - reward sum: -3578.2928 - l/s: -0.0060 - t: 278.6275\n",
      "1000/1000 [==============================] - 255s 255ms/step - loss: 0.0028 - mean q: 0.7255 - rewards: -0.0352 - reward sum: -3718.9384 - l/s: -0.0930 - t: 238.6142\n",
      "1000/1000 [==============================] - 255s 255ms/step - loss: 0.0023 - mean q: 0.7389 - rewards: -0.0209 - reward sum: -3802.6940 - l/s: -0.1010 - t: 248.1241\n",
      "1000/1000 [==============================] - 250s 250ms/step - loss: 0.0022 - mean q: 0.7921 - rewards: -0.0188 - reward sum: -3877.7233 - l/s: -0.1650 - t: 248.9590\n",
      "1000/1000 [==============================] - 252s 252ms/step - loss: 0.0021 - mean q: 0.8351 - rewards: -0.0111 - reward sum: -3922.0921 - l/s: -0.1240 - t: 275.0471\n",
      "1000/1000 [==============================] - 250s 250ms/step - loss: 0.0027 - mean q: 0.8395 - rewards: -0.0226 - reward sum: -4012.3965 - l/s: -0.0895 - t: 250.0902\n",
      "1000/1000 [==============================] - 252s 252ms/step - loss: 0.0021 - mean q: 0.8375 - rewards: -0.0165 - reward sum: -4078.4648 - l/s: 0.0305 - t: 252.7643\n",
      "1000/1000 [==============================] - 253s 253ms/step - loss: 0.0026 - mean q: 0.8554 - rewards: -0.0248 - reward sum: -4177.5760 - l/s: 0.0505 - t: 249.8630\n",
      "1000/1000 [==============================] - 254s 254ms/step - loss: 0.0022 - mean q: 0.8715 - rewards: -0.0129 - reward sum: -4228.9952 - l/s: 0.0240 - t: 252.4003\n",
      "1000/1000 [==============================] - 257s 257ms/step - loss: 0.0022 - mean q: 0.8756 - rewards: -0.0272 - reward sum: -4337.8629 - l/s: 0.0525 - t: 272.5845\n",
      "1000/1000 [==============================] - 257s 257ms/step - loss: 0.0016 - mean q: 0.8862 - rewards: -0.0217 - reward sum: -4424.6338 - l/s: 0.0310 - t: 261.1567\n",
      "1000/1000 [==============================] - 259s 259ms/step - loss: 0.0017 - mean q: 0.8855 - rewards: -0.0244 - reward sum: -4522.1927 - l/s: -0.0885 - t: 253.8033\n",
      "1000/1000 [==============================] - 260s 260ms/step - loss: 0.0017 - mean q: 0.8889 - rewards: -0.0411 - reward sum: -4686.6714 - l/s: 0.0380 - t: 255.6705\n",
      "1000/1000 [==============================] - 260s 260ms/step - loss: 0.0019 - mean q: 0.9244 - rewards: -0.0149 - reward sum: -4746.3559 - l/s: -0.0320 - t: 255.9215\n",
      "1000/1000 [==============================] - 265s 265ms/step - loss: 0.0019 - mean q: 0.9403 - rewards: -0.0160 - reward sum: -4810.2456 - l/s: 0.1305 - t: 265.5952\n",
      "1000/1000 [==============================] - 284s 285ms/step - loss: 0.0014 - mean q: 0.9563 - rewards: -0.0360 - reward sum: -4954.3399 - l/s: 0.0705 - t: 266.3601\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 0.0025 - mean q: 0.9791 - rewards: -0.0132 - reward sum: -5007.0515 - l/s: -0.0685 - t: 305.8372\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.0036 - mean q: 1.0124 - rewards: -0.0286 - reward sum: -5121.3104 - l/s: -0.0895 - t: 270.8442\n",
      "1000/1000 [==============================] - 293s 293ms/step - loss: 0.0019 - mean q: 1.0235 - rewards: -0.0108 - reward sum: -5164.5616 - l/s: -0.0860 - t: 310.7457\n",
      "1000/1000 [==============================] - 302s 302ms/step - loss: 0.0023 - mean q: 1.0458 - rewards: -0.0119 - reward sum: -5212.2246 - l/s: 0.0245 - t: 289.8833\n",
      "1000/1000 [==============================] - 294s 294ms/step - loss: 0.0020 - mean q: 1.0831 - rewards: -0.0293 - reward sum: -5329.3148 - l/s: 0.0580 - t: 279.5740\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.0027 - mean q: 1.0853 - rewards: -0.0128 - reward sum: -5380.5867 - l/s: -0.0385 - t: 284.3323\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.0030 - mean q: 1.0716 - rewards: -0.0163 - reward sum: -5445.6886 - l/s: -0.0900 - t: 287.2274\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.0019 - mean q: 1.0658 - rewards: -0.0304 - reward sum: -5567.2170 - l/s: -0.1040 - t: 319.6094\n",
      "1000/1000 [==============================] - 293s 293ms/step - loss: 0.0030 - mean q: 1.0698 - rewards: -0.0336 - reward sum: -5701.6506 - l/s: 0.0190 - t: 281.9445\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.0024 - mean q: 1.0839 - rewards: -0.0208 - reward sum: -5784.7445 - l/s: -0.0680 - t: 267.6342\n",
      "1000/1000 [==============================] - 266s 266ms/step - loss: 0.0030 - mean q: 1.0839 - rewards: -0.0302 - reward sum: -5905.4070 - l/s: 0.0225 - t: 257.6169\n",
      "1000/1000 [==============================] - 265s 265ms/step - loss: 0.0095 - mean q: 1.0936 - rewards: -0.0306 - reward sum: -6027.7335 - l/s: -0.0020 - t: 267.1262\n",
      "1000/1000 [==============================] - 266s 266ms/step - loss: 0.0030 - mean q: 1.0761 - rewards: -0.0147 - reward sum: -6086.4697 - l/s: -0.1165 - t: 260.2264\n",
      "1000/1000 [==============================] - 264s 264ms/step - loss: 0.0027 - mean q: 1.0593 - rewards: -0.0394 - reward sum: -6243.8907 - l/s: -0.1155 - t: 263.6848\n",
      "1000/1000 [==============================] - 268s 268ms/step - loss: 0.0023 - mean q: 1.0649 - rewards: -0.0239 - reward sum: -6339.4872 - l/s: -0.2014 - t: 266.2224\n",
      "1000/1000 [==============================] - 266s 266ms/step - loss: 0.0028 - mean q: 1.0913 - rewards: -0.0075 - reward sum: -6369.3673 - l/s: 0.0365 - t: 267.0447\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0034 - mean q: 1.0987 - rewards: -0.0294 - reward sum: -6486.9271 - l/s: 4.9988e-04 - t: 323.7147\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.0029 - mean q: 1.1028 - rewards: -0.0307 - reward sum: -6609.5306 - l/s: -0.0255 - t: 289.7915\n",
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.0025 - mean q: 1.1093 - rewards: -0.0305 - reward sum: -6731.5868 - l/s: 0.0335 - t: 313.8042\n",
      "1000/1000 [==============================] - 318s 318ms/step - loss: 0.0022 - mean q: 1.1010 - rewards: -0.0307 - reward sum: -6854.2946 - l/s: -0.0590 - t: 303.2485\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 0.0028 - mean q: 1.1133 - rewards: -0.0316 - reward sum: -6980.8710 - l/s: -0.1535 - t: 325.5106\n",
      "1000/1000 [==============================] - 327s 327ms/step - loss: 0.0031 - mean q: 1.1198 - rewards: -0.0190 - reward sum: -7056.7392 - l/s: -0.2454 - t: 291.4947\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.0053 - mean q: 1.0922 - rewards: -0.0403 - reward sum: -7217.8130 - l/s: -0.0670 - t: 285.8977\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.0024 - mean q: 1.0929 - rewards: -0.0214 - reward sum: -7303.5104 - l/s: -0.0350 - t: 311.6145\n",
      "1000/1000 [==============================] - 313s 313ms/step - loss: 0.0023 - mean q: 1.0887 - rewards: -0.0321 - reward sum: -7432.0575 - l/s: -0.0020 - t: 323.6516\n",
      "1000/1000 [==============================] - 341s 341ms/step - loss: 0.0019 - mean q: 1.0900 - rewards: -0.0403 - reward sum: -7593.1947 - l/s: -0.0360 - t: 347.9479\n",
      "1000/1000 [==============================] - 334s 334ms/step - loss: 0.0032 - mean q: 1.1146 - rewards: -0.0279 - reward sum: -7704.6095 - l/s: -0.0165 - t: 314.9221\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.0031 - mean q: 1.1356 - rewards: -0.0275 - reward sum: -7814.6839 - l/s: -0.1755 - t: 282.7607\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.0039 - mean q: 1.1318 - rewards: -0.0111 - reward sum: -7859.1977 - l/s: -0.1735 - t: 287.0502\n",
      "1000/1000 [==============================] - 292s 292ms/step - loss: 0.0035 - mean q: 1.1210 - rewards: -0.0300 - reward sum: -7979.0896 - l/s: -0.2369 - t: 314.4206\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 0.0024 - mean q: 1.1632 - rewards: -0.0135 - reward sum: -8033.0588 - l/s: -0.0495 - t: 281.2142\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.0022 - mean q: 1.1833 - rewards: -0.0185 - reward sum: -8106.9504 - l/s: -0.1005 - t: 287.9636\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.0025 - mean q: 1.1988 - rewards: -0.0091 - reward sum: -8143.2489 - l/s: -0.0515 - t: 278.0138\n",
      "1000/1000 [==============================] - 288s 289ms/step - loss: 0.0034 - mean q: 1.2288 - rewards: -0.0192 - reward sum: -8219.9679 - l/s: -0.0785 - t: 331.3700\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.0022 - mean q: 1.2502 - rewards: -0.0176 - reward sum: -8290.2871 - l/s: -0.0065 - t: 295.8514\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.0026 - mean q: 1.2610 - rewards: -0.0136 - reward sum: -8344.7624 - l/s: 0.0660 - t: 292.6876\n",
      "1000/1000 [==============================] - 319s 319ms/step - loss: 0.0030 - mean q: 1.2716 - rewards: -0.0143 - reward sum: -8402.0649 - l/s: -0.0015 - t: 282.0574\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.0024 - mean q: 1.2854 - rewards: -0.0255 - reward sum: -8503.9286 - l/s: -0.0155 - t: 274.7777\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0043 - mean q: 1.3182 - rewards: -0.0157 - reward sum: -8566.7417 - l/s: 0.1405 - t: 281.0712\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0026 - mean q: 1.3500 - rewards: -0.0244 - reward sum: -8664.4353 - l/s: 0.1385 - t: 277.7983\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0020 - mean q: 1.3723 - rewards: -0.0109 - reward sum: -8708.0220 - l/s: -0.0500 - t: 278.9421\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0017 - mean q: 1.3852 - rewards: -0.0236 - reward sum: -8802.3031 - l/s: -0.0330 - t: 275.9306\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0027 - mean q: 1.3885 - rewards: -0.0101 - reward sum: -8842.5215 - l/s: 0.1215 - t: 278.3751\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0020 - mean q: 1.4019 - rewards: -0.0338 - reward sum: -8977.8792 - l/s: -0.0425 - t: 275.9223\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0023 - mean q: 1.4305 - rewards: -0.0451 - reward sum: -9158.2976 - l/s: -0.0750 - t: 273.7000\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0032 - mean q: 1.4600 - rewards: -0.0375 - reward sum: -9308.3283 - l/s: -0.0560 - t: 273.2167\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0029 - mean q: 1.4891 - rewards: 2.6720e-04 - reward sum: -9307.2595 - l/s: -0.1590 - t: 277.0281\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0029 - mean q: 1.4967 - rewards: -0.0285 - reward sum: -9421.1947 - l/s: -0.0405 - t: 276.3381\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0059 - mean q: 1.5135 - rewards: -0.0398 - reward sum: -9580.5209 - l/s: 0.0120 - t: 275.9039\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.0057 - mean q: 1.5334 - rewards: -0.0258 - reward sum: -9683.7770 - l/s: 0.1285 - t: 308.0873\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 0.0024 - mean q: 1.5452 - rewards: -0.0306 - reward sum: -9806.1073 - l/s: 0.1340 - t: 329.6301\n",
      "1000/1000 [==============================] - 343s 343ms/step - loss: 0.0021 - mean q: 1.5556 - rewards: -0.0212 - reward sum: -9890.7641 - l/s: -0.0665 - t: 357.5227\n",
      "1000/1000 [==============================] - 351s 351ms/step - loss: 0.0017 - mean q: 1.5491 - rewards: -0.0212 - reward sum: -9975.7579 - l/s: -0.0495 - t: 345.2024\n",
      "1000/1000 [==============================] - 343s 343ms/step - loss: 0.0017 - mean q: 1.5514 - rewards: -0.0309 - reward sum: -10099.1731 - l/s: -0.0695 - t: 306.0636\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0023 - mean q: 1.5427 - rewards: -0.0134 - reward sum: -10152.6196 - l/s: 0.0780 - t: 275.4449\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0023 - mean q: 1.5485 - rewards: -0.0190 - reward sum: -10228.6810 - l/s: 0.0515 - t: 273.4232\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0026 - mean q: 1.5439 - rewards: -0.0190 - reward sum: -10304.5260 - l/s: 0.0565 - t: 279.6022\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0019 - mean q: 1.5598 - rewards: -0.0093 - reward sum: -10341.5371 - l/s: -0.0330 - t: 279.1319\n",
      "1000/1000 [==============================] - 276s 276ms/step - loss: 0.0026 - mean q: 1.5845 - rewards: -0.0069 - reward sum: -10369.2545 - l/s: -0.1835 - t: 265.3364\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0019 - mean q: 1.6003 - rewards: -0.0063 - reward sum: -10394.5220 - l/s: -0.1335 - t: 272.7542\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0018 - mean q: 1.5994 - rewards: -0.0137 - reward sum: -10449.4137 - l/s: -0.0680 - t: 277.4664\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0015 - mean q: 1.5951 - rewards: -0.0232 - reward sum: -10542.0280 - l/s: -0.1290 - t: 279.6464\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0012 - mean q: 1.6067 - rewards: -0.0112 - reward sum: -10586.6702 - l/s: -0.1785 - t: 277.5347\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0017 - mean q: 1.6267 - rewards: -0.0452 - reward sum: -10767.4945 - l/s: -0.0825 - t: 279.2855\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0015 - mean q: 1.6254 - rewards: -0.0205 - reward sum: -10849.6709 - l/s: -0.1645 - t: 278.4933\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0016 - mean q: 1.6110 - rewards: -0.0030 - reward sum: -10861.6545 - l/s: -0.0200 - t: 282.3898\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0020 - mean q: 1.5959 - rewards: 0.0132 - reward sum: -10808.9622 - l/s: 0.0025 - t: 278.9457\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0016 - mean q: 1.5954 - rewards: -0.0180 - reward sum: -10880.9953 - l/s: 0.0765 - t: 273.1907\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0016 - mean q: 1.5992 - rewards: -0.0043 - reward sum: -10898.2315 - l/s: -0.1760 - t: 277.5081\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0018 - mean q: 1.6054 - rewards: -0.0120 - reward sum: -10946.4228 - l/s: -0.0895 - t: 274.7894\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0015 - mean q: 1.6197 - rewards: -0.0157 - reward sum: -11009.2577 - l/s: -0.0605 - t: 273.4120\n",
      "1000/1000 [==============================] - 278s 277ms/step - loss: 0.0012 - mean q: 1.6223 - rewards: -0.0058 - reward sum: -11032.6147 - l/s: -0.0390 - t: 277.1675\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0016 - mean q: 1.6327 - rewards: -0.0065 - reward sum: -11058.7608 - l/s: -0.2429 - t: 274.4430\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0029 - mean q: 1.6280 - rewards: -0.0231 - reward sum: -11150.9753 - l/s: 0.0695 - t: 279.7666\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0024 - mean q: 1.6374 - rewards: -0.0073 - reward sum: -11180.2572 - l/s: -0.0180 - t: 287.4551\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0016 - mean q: 1.6449 - rewards: -0.0051 - reward sum: -11200.6204 - l/s: 0.0660 - t: 277.4002\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0018 - mean q: 1.6406 - rewards: -0.0320 - reward sum: -11328.4615 - l/s: -0.2239 - t: 280.2701\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.0015 - mean q: 1.6446 - rewards: -0.0167 - reward sum: -11395.3351 - l/s: -0.1665 - t: 278.2750\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0019 - mean q: 1.6397 - rewards: -0.0279 - reward sum: -11507.0417 - l/s: -0.1520 - t: 273.0774\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 0.0027 - mean q: 1.6389 - rewards: -0.0193 - reward sum: -11584.3235 - l/s: -0.1660 - t: 285.5583\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0012 - mean q: 1.6307 - rewards: -0.0177 - reward sum: -11655.1726 - l/s: -0.0670 - t: 271.2568\n",
      "1000/1000 [==============================] - 275s 275ms/step - loss: 0.0013 - mean q: 1.6276 - rewards: -0.0134 - reward sum: -11708.6486 - l/s: -0.1365 - t: 276.9902\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 7.7929e-04 - mean q: 1.6206 - rewards: -0.0157 - reward sum: -11771.6001 - l/s: -0.0620 - t: 274.1646\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0025 - mean q: 1.6147 - rewards: -0.0272 - reward sum: -11880.3547 - l/s: -0.0570 - t: 279.6809\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0017 - mean q: 1.6106 - rewards: -0.0160 - reward sum: -11944.3431 - l/s: -0.0705 - t: 280.5244\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0023 - mean q: 1.5991 - rewards: -0.0112 - reward sum: -11989.0581 - l/s: -0.0930 - t: 279.9237\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0016 - mean q: 1.5969 - rewards: -0.0220 - reward sum: -12076.8890 - l/s: -0.0220 - t: 274.1738\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0016 - mean q: 1.5813 - rewards: -0.0168 - reward sum: -12143.9073 - l/s: 9.9975e-04 - t: 285.6908\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0016 - mean q: 1.5906 - rewards: -0.0257 - reward sum: -12246.6106 - l/s: -0.1740 - t: 278.2462\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0017 - mean q: 1.5940 - rewards: -0.0181 - reward sum: -12319.1327 - l/s: 0.0220 - t: 278.3858\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0326 - mean q: 1.5798 - rewards: -0.0188 - reward sum: -12394.4205 - l/s: -0.0795 - t: 275.0374\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 9.9444e-04 - mean q: 1.5510 - rewards: -0.0152 - reward sum: -12455.0689 - l/s: -0.1440 - t: 306.3287\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 0.0018 - mean q: 1.5214 - rewards: -0.0075 - reward sum: -12484.9133 - l/s: -0.2219 - t: 280.9134\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0020 - mean q: 1.4691 - rewards: -0.0213 - reward sum: -12570.1994 - l/s: -0.0840 - t: 273.0010\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0015 - mean q: 1.4345 - rewards: 6.8799e-04 - reward sum: -12567.4474 - l/s: -0.1435 - t: 280.2331\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0011 - mean q: 1.4131 - rewards: -0.0251 - reward sum: -12667.7807 - l/s: -0.1195 - t: 281.9064\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 0.0012 - mean q: 1.3799 - rewards: -0.0282 - reward sum: -12780.7510 - l/s: -0.1080 - t: 278.9183\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0010 - mean q: 1.3585 - rewards: -0.0238 - reward sum: -12875.9858 - l/s: -0.0915 - t: 280.6330\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 0.0010 - mean q: 1.3533 - rewards: -0.0162 - reward sum: -12940.7192 - l/s: -0.0115 - t: 278.3353\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0014 - mean q: 1.3493 - rewards: -0.0301 - reward sum: -13061.0028 - l/s: 0.1330 - t: 283.0846\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0015 - mean q: 1.3429 - rewards: -0.0165 - reward sum: -13127.1170 - l/s: 0.1910 - t: 282.5505\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.0011 - mean q: 1.3447 - rewards: -0.0092 - reward sum: -13163.7959 - l/s: 0.0885 - t: 283.4616\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 7.7012e-04 - mean q: 1.3355 - rewards: -0.0146 - reward sum: -13222.0814 - l/s: 0.0975 - t: 278.7717\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.0015 - mean q: 1.3262 - rewards: -0.0200 - reward sum: -13301.9386 - l/s: 0.1070 - t: 282.1384\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.0014 - mean q: 1.2969 - rewards: -0.0097 - reward sum: -13340.7389 - l/s: 0.0640 - t: 278.2097\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.0013 - mean q: 1.2725 - rewards: -0.0121 - reward sum: -13389.2776 - l/s: 0.1020 - t: 384.4972\n",
      "1000/1000 [==============================] - 338s 338ms/step - loss: 0.0014 - mean q: 1.2707 - rewards: -0.0241 - reward sum: -13485.8557 - l/s: 0.0085 - t: 309.4681\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.0012 - mean q: 1.2587 - rewards: -0.0215 - reward sum: -13571.7636 - l/s: 0.1085 - t: 297.7141\n",
      "1000/1000 [==============================] - 318s 318ms/step - loss: 8.8965e-04 - mean q: 1.2388 - rewards: -0.0214 - reward sum: -13657.3707 - l/s: -0.0220 - t: 369.0644\n",
      "1000/1000 [==============================] - 342s 342ms/step - loss: 0.0013 - mean q: 1.2196 - rewards: -0.0215 - reward sum: -13743.3914 - l/s: 0.1380 - t: 355.1703\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 0.0011 - mean q: 1.2144 - rewards: -0.0071 - reward sum: -13771.7925 - l/s: 0.1565 - t: 301.8480\n",
      "1000/1000 [==============================] - 335s 336ms/step - loss: 0.0011 - mean q: 1.1999 - rewards: -0.0049 - reward sum: -13791.4984 - l/s: 0.0405 - t: 324.4018\n",
      "1000/1000 [==============================] - 331s 332ms/step - loss: 9.6677e-04 - mean q: 1.1779 - rewards: -0.0172 - reward sum: -13860.4766 - l/s: -0.0860 - t: 337.2368\n",
      "1000/1000 [==============================] - 336s 336ms/step - loss: 9.8828e-04 - mean q: 1.1559 - rewards: -0.0110 - reward sum: -13904.5955 - l/s: -0.0660 - t: 328.2297\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 8.1032e-04 - mean q: 1.1559 - rewards: -0.0148 - reward sum: -13963.6341 - l/s: -0.0285 - t: 325.3295\n",
      "1000/1000 [==============================] - 322s 322ms/step - loss: 0.0011 - mean q: 1.1477 - rewards: -0.0101 - reward sum: -14004.0349 - l/s: -0.1055 - t: 299.8286\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 9.3885e-04 - mean q: 1.1343 - rewards: -0.0234 - reward sum: -14097.6914 - l/s: -0.0650 - t: 290.5303\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 8.7524e-04 - mean q: 1.1077 - rewards: -0.0111 - reward sum: -14141.9992 - l/s: -0.0670 - t: 275.7244\n",
      "1000/1000 [==============================] - 289s 289ms/step - loss: 8.4344e-04 - mean q: 1.0922 - rewards: -0.0191 - reward sum: -14218.4994 - l/s: 0.0330 - t: 305.6075\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 8.7344e-04 - mean q: 1.0767 - rewards: -0.0125 - reward sum: -14268.5191 - l/s: -0.0385 - t: 282.9754\n",
      "1000/1000 [==============================] - 291s 291ms/step - loss: 9.6394e-04 - mean q: 1.0636 - rewards: -0.0042 - reward sum: -14285.5133 - l/s: 0.0785 - t: 291.4466\n",
      "1000/1000 [==============================] - 307s 307ms/step - loss: 0.0011 - mean q: 1.0937 - rewards: -0.0115 - reward sum: -14331.7131 - l/s: 0.1820 - t: 329.4340\n",
      "1000/1000 [==============================] - 323s 323ms/step - loss: 8.4990e-04 - mean q: 1.0846 - rewards: -0.0083 - reward sum: -14364.7938 - l/s: 0.1570 - t: 295.6810\n",
      "1000/1000 [==============================] - 331s 331ms/step - loss: 6.0126e-04 - mean q: 1.0649 - rewards: -0.0089 - reward sum: -14400.3564 - l/s: 0.0160 - t: 360.6014\n",
      "1000/1000 [==============================] - 327s 327ms/step - loss: 0.0012 - mean q: 1.0542 - rewards: -0.0018 - reward sum: -14407.4970 - l/s: -0.0090 - t: 292.1777\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 8.8087e-04 - mean q: 1.0564 - rewards: -0.0179 - reward sum: -14479.1586 - l/s: -0.0420 - t: 353.3477\n",
      "1000/1000 [==============================] - 318s 318ms/step - loss: 8.6374e-04 - mean q: 1.0441 - rewards: 0.0044 - reward sum: -14461.6486 - l/s: -4.9988e-04 - t: 298.6522\n",
      "1000/1000 [==============================] - 315s 316ms/step - loss: 6.9320e-04 - mean q: 1.0272 - rewards: -0.0203 - reward sum: -14542.7845 - l/s: -0.0015 - t: 297.8787\n",
      "1000/1000 [==============================] - 330s 330ms/step - loss: 9.6936e-04 - mean q: 1.0116 - rewards: -0.0128 - reward sum: -14594.0277 - l/s: 0.0215 - t: 368.6679\n",
      "1000/1000 [==============================] - 335s 335ms/step - loss: 9.0916e-04 - mean q: 1.0064 - rewards: -0.0150 - reward sum: -14653.8821 - l/s: 0.0015 - t: 339.0060\n",
      "1000/1000 [==============================] - 335s 335ms/step - loss: 7.5968e-04 - mean q: 1.0009 - rewards: -0.0030 - reward sum: -14665.9836 - l/s: 0.1450 - t: 336.4415\n",
      "1000/1000 [==============================] - 307s 307ms/step - loss: 7.9212e-04 - mean q: 0.9898 - rewards: -0.0063 - reward sum: -14691.1895 - l/s: -0.0055 - t: 297.5979\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 0.0015 - mean q: 0.9813 - rewards: -0.0218 - reward sum: -14778.3297 - l/s: -0.0110 - t: 287.6972\n",
      "1000/1000 [==============================] - 313s 313ms/step - loss: 0.0014 - mean q: 0.9597 - rewards: 0.0183 - reward sum: -14705.1615 - l/s: -0.0645 - t: 323.2100\n",
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.0663 - mean q: 0.9794 - rewards: -0.0197 - reward sum: -14784.1033 - l/s: 0.0260 - t: 334.4490\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.0025 - mean q: 0.9699 - rewards: -0.0252 - reward sum: -14885.0250 - l/s: -0.0565 - t: 287.8616\n",
      "1000/1000 [==============================] - 304s 304ms/step - loss: 0.0017 - mean q: 0.9721 - rewards: -0.0266 - reward sum: -14991.3745 - l/s: 0.0090 - t: 285.1645\n",
      "1000/1000 [==============================] - 312s 313ms/step - loss: 0.0015 - mean q: 0.9344 - rewards: -0.0300 - reward sum: -15111.5496 - l/s: -0.1235 - t: 298.6514\n",
      "1000/1000 [==============================] - 321s 321ms/step - loss: 0.0014 - mean q: 0.9272 - rewards: -0.0224 - reward sum: -15201.2500 - l/s: 0.0235 - t: 358.9371\n",
      "1000/1000 [==============================] - 364s 364ms/step - loss: 0.0032 - mean q: 0.9223 - rewards: 0.0157 - reward sum: -15138.5831 - l/s: 0.1625 - t: 359.2689\n",
      "1000/1000 [==============================] - 349s 349ms/step - loss: 0.0023 - mean q: 0.9234 - rewards: -0.0221 - reward sum: -15227.1457 - l/s: 0.2014 - t: 328.9416\n",
      "1000/1000 [==============================] - 338s 338ms/step - loss: 0.0013 - mean q: 0.9205 - rewards: -0.0137 - reward sum: -15282.0947 - l/s: 0.0655 - t: 342.7412\n",
      "1000/1000 [==============================] - 337s 337ms/step - loss: 0.0016 - mean q: 0.9178 - rewards: -0.0075 - reward sum: -15311.9988 - l/s: 0.0185 - t: 300.3725\n",
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.0018 - mean q: 0.9154 - rewards: -0.0226 - reward sum: -15402.3657 - l/s: 0.0780 - t: 335.6115\n",
      "1000/1000 [==============================] - 312s 312ms/step - loss: 0.0021 - mean q: 0.8973 - rewards: -0.0117 - reward sum: -15448.9808 - l/s: 0.0985 - t: 307.0138\n",
      "1000/1000 [==============================] - 319s 319ms/step - loss: 0.0016 - mean q: 0.8836 - rewards: -0.0153 - reward sum: -15510.1489 - l/s: 0.1000 - t: 285.5120\n",
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.0016 - mean q: 0.8841 - rewards: -0.0049 - reward sum: -15529.8132 - l/s: 0.1355 - t: 292.0598\n",
      "1000/1000 [==============================] - 309s 309ms/step - loss: 0.0013 - mean q: 0.8680 - rewards: -0.0118 - reward sum: -15577.1290 - l/s: 0.0715 - t: 314.5012\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.0017 - mean q: 0.8617 - rewards: 0.0091 - reward sum: -15540.6781 - l/s: -0.0655 - t: 318.0352\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 0.0015 - mean q: 0.8709 - rewards: -0.0234 - reward sum: -15634.3277 - l/s: 0.1265 - t: 296.1022\n",
      "1000/1000 [==============================] - 306s 306ms/step - loss: 0.0012 - mean q: 0.8624 - rewards: -0.0173 - reward sum: -15703.7189 - l/s: -0.0680 - t: 308.7624\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 0.0014 - mean q: 0.8579 - rewards: -0.0105 - reward sum: -15745.6326 - l/s: 0.0045 - t: 295.6146\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.0017 - mean q: 0.8582 - rewards: -0.0138 - reward sum: -15800.7337 - l/s: -0.0115 - t: 341.6827\n",
      "1000/1000 [==============================] - 307s 307ms/step - loss: 0.0012 - mean q: 0.8663 - rewards: -0.0080 - reward sum: -15832.8519 - l/s: 0.0770 - t: 288.5378\n",
      "1000/1000 [==============================] - 320s 320ms/step - loss: 0.0010 - mean q: 0.8701 - rewards: -0.0175 - reward sum: -15902.7105 - l/s: 0.0400 - t: 298.0278\n",
      "1000/1000 [==============================] - 322s 322ms/step - loss: 0.0012 - mean q: 0.8701 - rewards: -0.0055 - reward sum: -15924.5729 - l/s: 0.0620 - t: 327.7613\n",
      "1000/1000 [==============================] - 304s 304ms/step - loss: 0.0037 - mean q: 0.8682 - rewards: -0.0086 - reward sum: -15959.0785 - l/s: 0.0310 - t: 284.1328\n",
      "1000/1000 [==============================] - 306s 306ms/step - loss: 0.0012 - mean q: 0.8554 - rewards: -0.0118 - reward sum: -16006.2224 - l/s: 0.1035 - t: 294.0830\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.0013 - mean q: 0.8599 - rewards: -0.0016 - reward sum: -16012.5892 - l/s: 0.0110 - t: 291.3094\n",
      "1000/1000 [==============================] - 304s 303ms/step - loss: 0.0015 - mean q: 0.8656 - rewards: -0.0143 - reward sum: -16069.6462 - l/s: 0.0085 - t: 289.6927\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 9.3106e-04 - mean q: 0.8615 - rewards: -0.0166 - reward sum: -16136.1590 - l/s: -0.0115 - t: 305.3223\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.0015 - mean q: 0.8672 - rewards: -3.0486e-04 - reward sum: -16137.3785 - l/s: -0.0135 - t: 296.7469\n",
      "1000/1000 [==============================] - 309s 309ms/step - loss: 0.0019 - mean q: 0.8766 - rewards: -0.0231 - reward sum: -16229.8932 - l/s: -0.0295 - t: 308.2094\n",
      "1000/1000 [==============================] - 319s 319ms/step - loss: 0.0018 - mean q: 0.8874 - rewards: -0.0178 - reward sum: -16301.0777 - l/s: -0.0560 - t: 312.2731\n",
      "1000/1000 [==============================] - 306s 306ms/step - loss: 0.0010 - mean q: 0.8792 - rewards: -0.0113 - reward sum: -16346.2135 - l/s: -0.0150 - t: 304.0087\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.0015 - mean q: 0.8844 - rewards: -0.0169 - reward sum: -16413.7527 - l/s: -0.0670 - t: 323.6550\n",
      "1000/1000 [==============================] - 336s 336ms/step - loss: 9.2706e-04 - mean q: 0.8907 - rewards: -0.0151 - reward sum: -16474.1728 - l/s: 0.0120 - t: 342.9346\n",
      "1000/1000 [==============================] - 334s 334ms/step - loss: 0.0010 - mean q: 0.8920 - rewards: -0.0174 - reward sum: -16543.6602 - l/s: -0.0340 - t: 348.7098\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.0011 - mean q: 0.9016 - rewards: -0.0129 - reward sum: -16595.0719 - l/s: -0.0700 - t: 292.5515\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.0015 - mean q: 0.9066 - rewards: -0.0089 - reward sum: -16630.7750 - l/s: -0.1420 - t: 279.8316\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 0.0022 - mean q: 0.9133 - rewards: -0.0062 - reward sum: -16655.6028 - l/s: -0.1700 - t: 280.5970\n",
      "1000/1000 [==============================] - 323s 323ms/step - loss: 0.0011 - mean q: 0.9162 - rewards: -0.0183 - reward sum: -16728.9583 - l/s: -0.0545 - t: 313.9889\n",
      "1000/1000 [==============================] - 325s 325ms/step - loss: 9.5258e-04 - mean q: 0.9159 - rewards: 6.2875e-04 - reward sum: -16726.4433 - l/s: -0.0615 - t: 336.5183\n",
      "1000/1000 [==============================] - 333s 333ms/step - loss: 0.0015 - mean q: 0.9192 - rewards: 4.9350e-06 - reward sum: -16726.4235 - l/s: -0.0850 - t: 365.5054\n",
      "1000/1000 [==============================] - 341s 341ms/step - loss: 0.0012 - mean q: 0.9140 - rewards: -0.0133 - reward sum: -16779.5117 - l/s: -0.1200 - t: 303.4774\n",
      "1000/1000 [==============================] - 328s 328ms/step - loss: 0.0019 - mean q: 0.9123 - rewards: -0.0117 - reward sum: -16826.4898 - l/s: -0.1740 - t: 372.7017\n",
      "1000/1000 [==============================] - 329s 329ms/step - loss: 0.0020 - mean q: 0.9169 - rewards: -0.0323 - reward sum: -16955.6790 - l/s: -0.1495 - t: 345.3810\n",
      "1000/1000 [==============================] - 345s 345ms/step - loss: 0.0012 - mean q: 0.9251 - rewards: -0.0234 - reward sum: -17049.0864 - l/s: -0.0670 - t: 348.5396\n",
      " 954/1000 [===========================>..] - ETA: 15s - loss: 0.0011 - mean q: 0.9162 - rewards: -0.0071 - reward sum: -17076.0678 - l/s: -0.0886 - t: 363.1797"
     ]
    }
   ],
   "source": [
    "\n",
    "num_parallel = training_parallel\n",
    "envs = [environment() for _ in range(num_parallel)]\n",
    "\n",
    "\n",
    "print(\"training...\")\n",
    "\n",
    "n = 100000000\n",
    "agent.train(num_steps = n, envs = envs, warmup = 0, log_interval = 1000, train_steps_per_step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e0cf8bb-22fe-41af-8ed1-428df91e39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = agent.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc4b9a35-1045-4c99-bc9f-6708ba0f7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7316311f-cb67-498e-9172-1dcedfcfb1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
